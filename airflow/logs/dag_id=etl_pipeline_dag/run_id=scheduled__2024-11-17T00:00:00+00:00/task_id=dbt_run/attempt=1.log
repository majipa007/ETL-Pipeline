[2024-11-18T12:37:39.158+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-18T12:37:39.170+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_pipeline_dag.dbt_run scheduled__2024-11-17T00:00:00+00:00 [queued]>
[2024-11-18T12:37:39.175+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_pipeline_dag.dbt_run scheduled__2024-11-17T00:00:00+00:00 [queued]>
[2024-11-18T12:37:39.176+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 2
[2024-11-18T12:37:39.197+0000] {taskinstance.py:2889} INFO - Executing <Task(BashOperator): dbt_run> on 2024-11-17 00:00:00+00:00
[2024-11-18T12:37:39.201+0000] {standard_task_runner.py:72} INFO - Started process 143 to run task
[2024-11-18T12:37:39.203+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'etl_pipeline_dag', 'dbt_run', 'scheduled__2024-11-17T00:00:00+00:00', '--job-id', '16', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpa82i8awf']
[2024-11-18T12:37:39.204+0000] {standard_task_runner.py:105} INFO - Job 16: Subtask dbt_run
[2024-11-18T12:37:39.247+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_pipeline_dag.dbt_run scheduled__2024-11-17T00:00:00+00:00 [running]> on host 945969587f65
[2024-11-18T12:37:39.424+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='dbt_run' AIRFLOW_CTX_EXECUTION_DATE='2024-11-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-17T00:00:00+00:00'
[2024-11-18T12:37:39.426+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-18T12:37:39.494+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2024-11-18T12:37:39.497+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', '\n            cd /opt/airflow/etl_pipeline &&             echo "Running dbt with profiles.yml in: $DBT_PROFILES_DIR" &&             dbt run --debug --profiles-dir /opt/airflow/etl_pipeline\n        ']
[2024-11-18T12:37:39.520+0000] {subprocess.py:99} INFO - Output:
[2024-11-18T12:37:39.525+0000] {subprocess.py:106} INFO - Running dbt with profiles.yml in:
[2024-11-18T12:37:41.461+0000] {subprocess.py:106} INFO - [0m12:37:41  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb919a60d70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9186783b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb91a9ffd10>]}
[2024-11-18T12:37:41.463+0000] {subprocess.py:106} INFO - [0m12:37:41  Running with dbt=1.8.8
[2024-11-18T12:37:41.464+0000] {subprocess.py:106} INFO - [0m12:37:41  running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/etl_pipeline', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/opt/airflow/etl_pipeline/logs', 'debug': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --debug --profiles-dir /opt/airflow/etl_pipeline', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[2024-11-18T12:37:42.013+0000] {subprocess.py:106} INFO - [0m12:37:42  Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb917c37560>]}
[2024-11-18T12:37:42.319+0000] {subprocess.py:106} INFO - [0m12:37:42  Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb917c37a10>]}
[2024-11-18T12:37:42.322+0000] {subprocess.py:106} INFO - [0m12:37:42  Registered adapter: postgres=1.8.2
[2024-11-18T12:37:42.416+0000] {subprocess.py:106} INFO - [0m12:37:42  checksum: ec53e341a936509427e16df1c646386a84befec52c1957e3f4359fbc36fc93f5, vars: {}, profile: , target: , version: 1.8.8
[2024-11-18T12:37:43.318+0000] {subprocess.py:106} INFO - [0m12:37:43  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[2024-11-18T12:37:43.325+0000] {subprocess.py:106} INFO - [0m12:37:43  Partial parsing enabled, no changes found, skipping parsing
[2024-11-18T12:37:43.370+0000] {subprocess.py:106} INFO - [0m12:37:43  [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
[2024-11-18T12:37:43.372+0000] {subprocess.py:106} INFO - There are 1 unused configuration paths:
[2024-11-18T12:37:43.372+0000] {subprocess.py:106} INFO - - models.etl_pipeline.production
[2024-11-18T12:37:43.630+0000] {subprocess.py:106} INFO - [0m12:37:43  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9181624b0>]}
[2024-11-18T12:37:43.999+0000] {subprocess.py:106} INFO - [0m12:37:43  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb91753d4c0>]}
[2024-11-18T12:37:44.000+0000] {subprocess.py:106} INFO - [0m12:37:44  Found 6 models, 26 data tests, 2 sources, 539 macros
[2024-11-18T12:37:44.001+0000] {subprocess.py:106} INFO - [0m12:37:44  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb918208470>]}
[2024-11-18T12:37:44.012+0000] {subprocess.py:106} INFO - [0m12:37:44
[2024-11-18T12:37:44.014+0000] {subprocess.py:106} INFO - [0m12:37:44  Acquiring new postgres connection 'master'
[2024-11-18T12:37:44.028+0000] {subprocess.py:106} INFO - [0m12:37:44  Acquiring new postgres connection 'list_db'
[2024-11-18T12:37:44.197+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "list_db"
[2024-11-18T12:37:44.198+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db"} */
[2024-11-18T12:37:44.198+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.199+0000] {subprocess.py:106} INFO -     select distinct nspname from pg_namespace
[2024-11-18T12:37:44.199+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.199+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state init
[2024-11-18T12:37:44.294+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 6 in 0.093 seconds
[2024-11-18T12:37:44.304+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db: Close
[2024-11-18T12:37:44.312+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "list_db"
[2024-11-18T12:37:44.313+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db"} */
[2024-11-18T12:37:44.313+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.313+0000] {subprocess.py:106} INFO -     select distinct nspname from pg_namespace
[2024-11-18T12:37:44.314+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.316+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state closed
[2024-11-18T12:37:44.349+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 6 in 0.031 seconds
[2024-11-18T12:37:44.353+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db: Close
[2024-11-18T12:37:44.364+0000] {subprocess.py:106} INFO - [0m12:37:44  Re-using an available connection from the pool (formerly list_db, now list_db_public_analytics_schema)
[2024-11-18T12:37:44.394+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "list_db_public_analytics_schema"
[2024-11-18T12:37:44.396+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public_analytics_schema: BEGIN
[2024-11-18T12:37:44.398+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state closed
[2024-11-18T12:37:44.415+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: BEGIN in 0.016 seconds
[2024-11-18T12:37:44.416+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "list_db_public_analytics_schema"
[2024-11-18T12:37:44.417+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public_analytics_schema: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db_public_analytics_schema"} */
[2024-11-18T12:37:44.417+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:44.417+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:37:44.417+0000] {subprocess.py:106} INFO -       tablename as name,
[2024-11-18T12:37:44.417+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:37:44.418+0000] {subprocess.py:106} INFO -       'table' as type
[2024-11-18T12:37:44.418+0000] {subprocess.py:106} INFO -     from pg_tables
[2024-11-18T12:37:44.418+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:37:44.418+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:37:44.419+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.419+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:37:44.419+0000] {subprocess.py:106} INFO -       viewname as name,
[2024-11-18T12:37:44.419+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:37:44.419+0000] {subprocess.py:106} INFO -       'view' as type
[2024-11-18T12:37:44.420+0000] {subprocess.py:106} INFO -     from pg_views
[2024-11-18T12:37:44.420+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:37:44.420+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:37:44.420+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.420+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:37:44.420+0000] {subprocess.py:106} INFO -       matviewname as name,
[2024-11-18T12:37:44.421+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:37:44.425+0000] {subprocess.py:106} INFO -       'materialized_view' as type
[2024-11-18T12:37:44.426+0000] {subprocess.py:106} INFO -     from pg_matviews
[2024-11-18T12:37:44.427+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:37:44.427+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.444+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 5 in 0.026 seconds
[2024-11-18T12:37:44.450+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public_analytics_schema: ROLLBACK
[2024-11-18T12:37:44.458+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public_analytics_schema: Close
[2024-11-18T12:37:44.466+0000] {subprocess.py:106} INFO - [0m12:37:44  Re-using an available connection from the pool (formerly list_db_public_analytics_schema, now list_db_public)
[2024-11-18T12:37:44.479+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "list_db_public"
[2024-11-18T12:37:44.480+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public: BEGIN
[2024-11-18T12:37:44.482+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state closed
[2024-11-18T12:37:44.496+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: BEGIN in 0.017 seconds
[2024-11-18T12:37:44.498+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "list_db_public"
[2024-11-18T12:37:44.498+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db_public"} */
[2024-11-18T12:37:44.499+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:44.499+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:37:44.499+0000] {subprocess.py:106} INFO -       tablename as name,
[2024-11-18T12:37:44.499+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:37:44.499+0000] {subprocess.py:106} INFO -       'table' as type
[2024-11-18T12:37:44.500+0000] {subprocess.py:106} INFO -     from pg_tables
[2024-11-18T12:37:44.500+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:37:44.500+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:37:44.501+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.501+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:37:44.502+0000] {subprocess.py:106} INFO -       viewname as name,
[2024-11-18T12:37:44.502+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:37:44.502+0000] {subprocess.py:106} INFO -       'view' as type
[2024-11-18T12:37:44.502+0000] {subprocess.py:106} INFO -     from pg_views
[2024-11-18T12:37:44.503+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:37:44.503+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:37:44.503+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.503+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:37:44.503+0000] {subprocess.py:106} INFO -       matviewname as name,
[2024-11-18T12:37:44.504+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:37:44.504+0000] {subprocess.py:106} INFO -       'materialized_view' as type
[2024-11-18T12:37:44.505+0000] {subprocess.py:106} INFO -     from pg_matviews
[2024-11-18T12:37:44.506+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:37:44.507+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.507+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 1 in 0.005 seconds
[2024-11-18T12:37:44.507+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public: ROLLBACK
[2024-11-18T12:37:44.512+0000] {subprocess.py:106} INFO - [0m12:37:44  On list_db_public: Close
[2024-11-18T12:37:44.526+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "master"
[2024-11-18T12:37:44.527+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: BEGIN
[2024-11-18T12:37:44.527+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state init
[2024-11-18T12:37:44.546+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: BEGIN in 0.015 seconds
[2024-11-18T12:37:44.548+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "master"
[2024-11-18T12:37:44.548+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "master"} */
[2024-11-18T12:37:44.549+0000] {subprocess.py:106} INFO - with relation as (
[2024-11-18T12:37:44.549+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:37:44.549+0000] {subprocess.py:106} INFO -             pg_rewrite.ev_class as class,
[2024-11-18T12:37:44.549+0000] {subprocess.py:106} INFO -             pg_rewrite.oid as id
[2024-11-18T12:37:44.550+0000] {subprocess.py:106} INFO -         from pg_rewrite
[2024-11-18T12:37:44.550+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:37:44.550+0000] {subprocess.py:106} INFO -     class as (
[2024-11-18T12:37:44.550+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:37:44.552+0000] {subprocess.py:106} INFO -             oid as id,
[2024-11-18T12:37:44.552+0000] {subprocess.py:106} INFO -             relname as name,
[2024-11-18T12:37:44.552+0000] {subprocess.py:106} INFO -             relnamespace as schema,
[2024-11-18T12:37:44.553+0000] {subprocess.py:106} INFO -             relkind as kind
[2024-11-18T12:37:44.553+0000] {subprocess.py:106} INFO -         from pg_class
[2024-11-18T12:37:44.553+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:37:44.553+0000] {subprocess.py:106} INFO -     dependency as (
[2024-11-18T12:37:44.553+0000] {subprocess.py:106} INFO -         select distinct
[2024-11-18T12:37:44.554+0000] {subprocess.py:106} INFO -             pg_depend.objid as id,
[2024-11-18T12:37:44.554+0000] {subprocess.py:106} INFO -             pg_depend.refobjid as ref
[2024-11-18T12:37:44.554+0000] {subprocess.py:106} INFO -         from pg_depend
[2024-11-18T12:37:44.554+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:37:44.555+0000] {subprocess.py:106} INFO -     schema as (
[2024-11-18T12:37:44.555+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:37:44.555+0000] {subprocess.py:106} INFO -             pg_namespace.oid as id,
[2024-11-18T12:37:44.555+0000] {subprocess.py:106} INFO -             pg_namespace.nspname as name
[2024-11-18T12:37:44.557+0000] {subprocess.py:106} INFO -         from pg_namespace
[2024-11-18T12:37:44.558+0000] {subprocess.py:106} INFO -         where nspname != 'information_schema' and nspname not like 'pg\_%'
[2024-11-18T12:37:44.558+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:37:44.561+0000] {subprocess.py:106} INFO -     referenced as (
[2024-11-18T12:37:44.562+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:37:44.563+0000] {subprocess.py:106} INFO -             relation.id AS id,
[2024-11-18T12:37:44.563+0000] {subprocess.py:106} INFO -             referenced_class.name ,
[2024-11-18T12:37:44.563+0000] {subprocess.py:106} INFO -             referenced_class.schema ,
[2024-11-18T12:37:44.564+0000] {subprocess.py:106} INFO -             referenced_class.kind
[2024-11-18T12:37:44.564+0000] {subprocess.py:106} INFO -         from relation
[2024-11-18T12:37:44.564+0000] {subprocess.py:106} INFO -         join class as referenced_class on relation.class=referenced_class.id
[2024-11-18T12:37:44.564+0000] {subprocess.py:106} INFO -         where referenced_class.kind in ('r', 'v', 'm')
[2024-11-18T12:37:44.565+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:37:44.565+0000] {subprocess.py:106} INFO -     relationships as (
[2024-11-18T12:37:44.565+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:37:44.565+0000] {subprocess.py:106} INFO -             referenced.name as referenced_name,
[2024-11-18T12:37:44.565+0000] {subprocess.py:106} INFO -             referenced.schema as referenced_schema_id,
[2024-11-18T12:37:44.567+0000] {subprocess.py:106} INFO -             dependent_class.name as dependent_name,
[2024-11-18T12:37:44.568+0000] {subprocess.py:106} INFO -             dependent_class.schema as dependent_schema_id,
[2024-11-18T12:37:44.568+0000] {subprocess.py:106} INFO -             referenced.kind as kind
[2024-11-18T12:37:44.569+0000] {subprocess.py:106} INFO -         from referenced
[2024-11-18T12:37:44.569+0000] {subprocess.py:106} INFO -         join dependency on referenced.id=dependency.id
[2024-11-18T12:37:44.569+0000] {subprocess.py:106} INFO -         join class as dependent_class on dependency.ref=dependent_class.id
[2024-11-18T12:37:44.569+0000] {subprocess.py:106} INFO -         where
[2024-11-18T12:37:44.569+0000] {subprocess.py:106} INFO -             (referenced.name != dependent_class.name or
[2024-11-18T12:37:44.570+0000] {subprocess.py:106} INFO -              referenced.schema != dependent_class.schema)
[2024-11-18T12:37:44.570+0000] {subprocess.py:106} INFO -     )
[2024-11-18T12:37:44.570+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.570+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.571+0000] {subprocess.py:106} INFO -         referenced_schema.name as referenced_schema,
[2024-11-18T12:37:44.571+0000] {subprocess.py:106} INFO -         relationships.referenced_name as referenced_name,
[2024-11-18T12:37:44.571+0000] {subprocess.py:106} INFO -         dependent_schema.name as dependent_schema,
[2024-11-18T12:37:44.571+0000] {subprocess.py:106} INFO -         relationships.dependent_name as dependent_name
[2024-11-18T12:37:44.572+0000] {subprocess.py:106} INFO -     from relationships
[2024-11-18T12:37:44.572+0000] {subprocess.py:106} INFO -     join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
[2024-11-18T12:37:44.574+0000] {subprocess.py:106} INFO -     join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
[2024-11-18T12:37:44.574+0000] {subprocess.py:106} INFO -     group by referenced_schema, referenced_name, dependent_schema, dependent_name
[2024-11-18T12:37:44.575+0000] {subprocess.py:106} INFO -     order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[2024-11-18T12:37:44.575+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 0 in 0.013 seconds
[2024-11-18T12:37:44.575+0000] {subprocess.py:106} INFO - [0m12:37:44  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb917c52d80>]}
[2024-11-18T12:37:44.576+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: ROLLBACK
[2024-11-18T12:37:44.578+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "master"
[2024-11-18T12:37:44.582+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: BEGIN
[2024-11-18T12:37:44.583+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: BEGIN in 0.000 seconds
[2024-11-18T12:37:44.587+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: COMMIT
[2024-11-18T12:37:44.588+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "master"
[2024-11-18T12:37:44.588+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: COMMIT
[2024-11-18T12:37:44.588+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: COMMIT in 0.000 seconds
[2024-11-18T12:37:44.589+0000] {subprocess.py:106} INFO - [0m12:37:44  On master: Close
[2024-11-18T12:37:44.589+0000] {subprocess.py:106} INFO - [0m12:37:44  Concurrency: 1 threads (target='dev')
[2024-11-18T12:37:44.589+0000] {subprocess.py:106} INFO - [0m12:37:44
[2024-11-18T12:37:44.595+0000] {subprocess.py:106} INFO - [0m12:37:44  Began running node model.etl_pipeline.dim_branches
[2024-11-18T12:37:44.596+0000] {subprocess.py:106} INFO - [0m12:37:44  1 of 6 START sql table model public_analytics_schema.dim_branches .............. [RUN]
[2024-11-18T12:37:44.597+0000] {subprocess.py:106} INFO - [0m12:37:44  Re-using an available connection from the pool (formerly list_db_public, now model.etl_pipeline.dim_branches)
[2024-11-18T12:37:44.598+0000] {subprocess.py:106} INFO - [0m12:37:44  Began compiling node model.etl_pipeline.dim_branches
[2024-11-18T12:37:44.687+0000] {subprocess.py:106} INFO - [0m12:37:44  Writing injected SQL for node "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.690+0000] {subprocess.py:106} INFO - [0m12:37:44  Began executing node model.etl_pipeline.dim_branches
[2024-11-18T12:37:44.758+0000] {subprocess.py:106} INFO - [0m12:37:44  Writing runtime sql for node "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.759+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.760+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: BEGIN
[2024-11-18T12:37:44.761+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state closed
[2024-11-18T12:37:44.771+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: BEGIN in 0.009 seconds
[2024-11-18T12:37:44.771+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.771+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:37:44.771+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.772+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.772+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.772+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.772+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_branches__dbt_tmp"
[2024-11-18T12:37:44.772+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.772+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.773+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:37:44.773+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.773+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:37:44.773+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.773+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.773+0000] {subprocess.py:106} INFO - -- Create the doctors dimension table in the analytics_schema
[2024-11-18T12:37:44.774+0000] {subprocess.py:106} INFO - with dim_bra as (
[2024-11-18T12:37:44.774+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.774+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as branch_id,
[2024-11-18T12:37:44.774+0000] {subprocess.py:106} INFO -         hospital_branch,
[2024-11-18T12:37:44.774+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:37:44.774+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:37:44.775+0000] {subprocess.py:106} INFO -     group by branch_id, hospital_branch
[2024-11-18T12:37:44.775+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:37:44.775+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:44.775+0000] {subprocess.py:106} INFO -     branch_id,
[2024-11-18T12:37:44.776+0000] {subprocess.py:106} INFO -     hospital_branch,
[2024-11-18T12:37:44.776+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:37:44.776+0000] {subprocess.py:106} INFO - from dim_bra
[2024-11-18T12:37:44.776+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:37:44.776+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.802+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 3 in 0.030 seconds
[2024-11-18T12:37:44.811+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.812+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:37:44.813+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_branches" rename to "dim_branches__dbt_backup"
[2024-11-18T12:37:44.814+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:44.819+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.819+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:37:44.819+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_branches__dbt_tmp" rename to "dim_branches"
[2024-11-18T12:37:44.820+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: ALTER TABLE in 0.000 seconds
[2024-11-18T12:37:44.846+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: COMMIT
[2024-11-18T12:37:44.847+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.847+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: COMMIT
[2024-11-18T12:37:44.850+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: COMMIT in 0.003 seconds
[2024-11-18T12:37:44.857+0000] {subprocess.py:106} INFO - [0m12:37:44  Applying DROP to: "db"."public_analytics_schema"."dim_branches__dbt_backup"
[2024-11-18T12:37:44.866+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:37:44.867+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:37:44.867+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_branches__dbt_backup" cascade
[2024-11-18T12:37:44.873+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: DROP TABLE in 0.005 seconds
[2024-11-18T12:37:44.876+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_branches: Close
[2024-11-18T12:37:44.879+0000] {subprocess.py:106} INFO - [0m12:37:44  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb919ae3b30>]}
[2024-11-18T12:37:44.879+0000] {subprocess.py:106} INFO - [0m12:37:44  1 of 6 OK created sql table model public_analytics_schema.dim_branches ......... [[32mSELECT 3[0m in 0.28s]
[2024-11-18T12:37:44.880+0000] {subprocess.py:106} INFO - [0m12:37:44  Finished running node model.etl_pipeline.dim_branches
[2024-11-18T12:37:44.880+0000] {subprocess.py:106} INFO - [0m12:37:44  Began running node model.etl_pipeline.dim_doctors
[2024-11-18T12:37:44.882+0000] {subprocess.py:106} INFO - [0m12:37:44  2 of 6 START sql table model public_analytics_schema.dim_doctors ............... [RUN]
[2024-11-18T12:37:44.883+0000] {subprocess.py:106} INFO - [0m12:37:44  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_branches, now model.etl_pipeline.dim_doctors)
[2024-11-18T12:37:44.884+0000] {subprocess.py:106} INFO - [0m12:37:44  Began compiling node model.etl_pipeline.dim_doctors
[2024-11-18T12:37:44.890+0000] {subprocess.py:106} INFO - [0m12:37:44  Writing injected SQL for node "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.891+0000] {subprocess.py:106} INFO - [0m12:37:44  Began executing node model.etl_pipeline.dim_doctors
[2024-11-18T12:37:44.900+0000] {subprocess.py:106} INFO - [0m12:37:44  Writing runtime sql for node "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.901+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.902+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: BEGIN
[2024-11-18T12:37:44.902+0000] {subprocess.py:106} INFO - [0m12:37:44  Opening a new connection, currently in state closed
[2024-11-18T12:37:44.912+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: BEGIN in 0.009 seconds
[2024-11-18T12:37:44.913+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.914+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:37:44.914+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.914+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.914+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.914+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.915+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_doctors__dbt_tmp"
[2024-11-18T12:37:44.915+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.915+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.915+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:37:44.915+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.915+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:37:44.916+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.916+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.916+0000] {subprocess.py:106} INFO - -- Create the doctors dimension table in the analytics_schema
[2024-11-18T12:37:44.916+0000] {subprocess.py:106} INFO - with dim_doc as (
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(doctor as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(department as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as doctor_id,
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -         doctor,
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -         department,
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -         count(*) as total_visits,
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:37:44.917+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:37:44.918+0000] {subprocess.py:106} INFO -     group by doctor_id, doctor, department
[2024-11-18T12:37:44.918+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:37:44.918+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:44.918+0000] {subprocess.py:106} INFO -     doctor_id,
[2024-11-18T12:37:44.918+0000] {subprocess.py:106} INFO -     doctor,
[2024-11-18T12:37:44.918+0000] {subprocess.py:106} INFO -     department,
[2024-11-18T12:37:44.919+0000] {subprocess.py:106} INFO -     total_visits,
[2024-11-18T12:37:44.919+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:37:44.919+0000] {subprocess.py:106} INFO - from dim_doc
[2024-11-18T12:37:44.919+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:37:44.919+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:44.936+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: SELECT 31 in 0.022 seconds
[2024-11-18T12:37:44.941+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.941+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:37:44.942+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_doctors" rename to "dim_doctors__dbt_backup"
[2024-11-18T12:37:44.943+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:44.950+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.950+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:37:44.951+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_doctors__dbt_tmp" rename to "dim_doctors"
[2024-11-18T12:37:44.952+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:44.954+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: COMMIT
[2024-11-18T12:37:44.955+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.956+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: COMMIT
[2024-11-18T12:37:44.959+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: COMMIT in 0.002 seconds
[2024-11-18T12:37:44.967+0000] {subprocess.py:106} INFO - [0m12:37:44  Applying DROP to: "db"."public_analytics_schema"."dim_doctors__dbt_backup"
[2024-11-18T12:37:44.969+0000] {subprocess.py:106} INFO - [0m12:37:44  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:37:44.969+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:37:44.970+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_doctors__dbt_backup" cascade
[2024-11-18T12:37:44.976+0000] {subprocess.py:106} INFO - [0m12:37:44  SQL status: DROP TABLE in 0.004 seconds
[2024-11-18T12:37:44.982+0000] {subprocess.py:106} INFO - [0m12:37:44  On model.etl_pipeline.dim_doctors: Close
[2024-11-18T12:37:44.985+0000] {subprocess.py:106} INFO - [0m12:37:44  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb916ba3710>]}
[2024-11-18T12:37:44.988+0000] {subprocess.py:106} INFO - [0m12:37:44  2 of 6 OK created sql table model public_analytics_schema.dim_doctors .......... [[32mSELECT 31[0m in 0.10s]
[2024-11-18T12:37:44.991+0000] {subprocess.py:106} INFO - [0m12:37:44  Finished running node model.etl_pipeline.dim_doctors
[2024-11-18T12:37:44.993+0000] {subprocess.py:106} INFO - [0m12:37:44  Began running node model.etl_pipeline.dim_patients
[2024-11-18T12:37:44.998+0000] {subprocess.py:106} INFO - [0m12:37:44  3 of 6 START sql table model public_analytics_schema.dim_patients .............. [RUN]
[2024-11-18T12:37:45.000+0000] {subprocess.py:106} INFO - [0m12:37:45  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_doctors, now model.etl_pipeline.dim_patients)
[2024-11-18T12:37:45.002+0000] {subprocess.py:106} INFO - [0m12:37:45  Began compiling node model.etl_pipeline.dim_patients
[2024-11-18T12:37:45.018+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing injected SQL for node "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.020+0000] {subprocess.py:106} INFO - [0m12:37:45  Began executing node model.etl_pipeline.dim_patients
[2024-11-18T12:37:45.037+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing runtime sql for node "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.039+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.041+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: BEGIN
[2024-11-18T12:37:45.044+0000] {subprocess.py:106} INFO - [0m12:37:45  Opening a new connection, currently in state closed
[2024-11-18T12:37:45.074+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: BEGIN in 0.030 seconds
[2024-11-18T12:37:45.076+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.079+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:37:45.081+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.081+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.082+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.083+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.083+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_patients__dbt_tmp"
[2024-11-18T12:37:45.084+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.084+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.085+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:37:45.086+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.087+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:37:45.087+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.088+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.088+0000] {subprocess.py:106} INFO - -- Create the patients dimension table in the analytics_schema
[2024-11-18T12:37:45.089+0000] {subprocess.py:106} INFO - with dim_pat as (
[2024-11-18T12:37:45.090+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:45.090+0000] {subprocess.py:106} INFO -         patient_id,
[2024-11-18T12:37:45.091+0000] {subprocess.py:106} INFO -         patient_name,
[2024-11-18T12:37:45.091+0000] {subprocess.py:106} INFO -         count(*) as total_visits,
[2024-11-18T12:37:45.092+0000] {subprocess.py:106} INFO -         sum(revenue) as total_spent
[2024-11-18T12:37:45.092+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:37:45.093+0000] {subprocess.py:106} INFO -     group by patient_id, patient_name
[2024-11-18T12:37:45.094+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:37:45.096+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:45.097+0000] {subprocess.py:106} INFO -     patient_id,
[2024-11-18T12:37:45.098+0000] {subprocess.py:106} INFO -     patient_name,
[2024-11-18T12:37:45.099+0000] {subprocess.py:106} INFO -     total_visits,
[2024-11-18T12:37:45.100+0000] {subprocess.py:106} INFO -     total_spent
[2024-11-18T12:37:45.100+0000] {subprocess.py:106} INFO - from dim_pat
[2024-11-18T12:37:45.101+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:37:45.102+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.107+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: SELECT 2423 in 0.025 seconds
[2024-11-18T12:37:45.125+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.128+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:37:45.128+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_patients" rename to "dim_patients__dbt_backup"
[2024-11-18T12:37:45.131+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:45.149+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.151+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:37:45.153+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_patients__dbt_tmp" rename to "dim_patients"
[2024-11-18T12:37:45.155+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.002 seconds
[2024-11-18T12:37:45.164+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: COMMIT
[2024-11-18T12:37:45.167+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.170+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: COMMIT
[2024-11-18T12:37:45.176+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: COMMIT in 0.004 seconds
[2024-11-18T12:37:45.188+0000] {subprocess.py:106} INFO - [0m12:37:45  Applying DROP to: "db"."public_analytics_schema"."dim_patients__dbt_backup"
[2024-11-18T12:37:45.191+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:37:45.193+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:37:45.194+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_patients__dbt_backup" cascade
[2024-11-18T12:37:45.201+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: DROP TABLE in 0.006 seconds
[2024-11-18T12:37:45.207+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_patients: Close
[2024-11-18T12:37:45.211+0000] {subprocess.py:106} INFO - [0m12:37:45  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb916fb0ce0>]}
[2024-11-18T12:37:45.215+0000] {subprocess.py:106} INFO - [0m12:37:45  3 of 6 OK created sql table model public_analytics_schema.dim_patients ......... [[32mSELECT 2423[0m in 0.21s]
[2024-11-18T12:37:45.218+0000] {subprocess.py:106} INFO - [0m12:37:45  Finished running node model.etl_pipeline.dim_patients
[2024-11-18T12:37:45.221+0000] {subprocess.py:106} INFO - [0m12:37:45  Began running node model.etl_pipeline.dim_risk
[2024-11-18T12:37:45.224+0000] {subprocess.py:106} INFO - [0m12:37:45  4 of 6 START sql table model public_analytics_schema.dim_risk .................. [RUN]
[2024-11-18T12:37:45.227+0000] {subprocess.py:106} INFO - [0m12:37:45  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_patients, now model.etl_pipeline.dim_risk)
[2024-11-18T12:37:45.229+0000] {subprocess.py:106} INFO - [0m12:37:45  Began compiling node model.etl_pipeline.dim_risk
[2024-11-18T12:37:45.249+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing injected SQL for node "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.251+0000] {subprocess.py:106} INFO - [0m12:37:45  Began executing node model.etl_pipeline.dim_risk
[2024-11-18T12:37:45.274+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing runtime sql for node "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.277+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.279+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: BEGIN
[2024-11-18T12:37:45.280+0000] {subprocess.py:106} INFO - [0m12:37:45  Opening a new connection, currently in state closed
[2024-11-18T12:37:45.310+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: BEGIN in 0.029 seconds
[2024-11-18T12:37:45.313+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.315+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:37:45.316+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.317+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.318+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.318+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.319+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_risk__dbt_tmp"
[2024-11-18T12:37:45.320+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.320+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.321+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:37:45.321+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.322+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:37:45.323+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.323+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.324+0000] {subprocess.py:106} INFO - -- Create the patient risk dimension table in the analytics_schema
[2024-11-18T12:37:45.325+0000] {subprocess.py:106} INFO - with dim_risk as (
[2024-11-18T12:37:45.325+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:45.326+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(patient_risk_profile as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as risk_id,
[2024-11-18T12:37:45.327+0000] {subprocess.py:106} INFO -         patient_risk_profile,  -- Ensure this column exists in the source table
[2024-11-18T12:37:45.327+0000] {subprocess.py:106} INFO -         count(*) as cases,
[2024-11-18T12:37:45.328+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:37:45.328+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:37:45.329+0000] {subprocess.py:106} INFO -     group by risk_id, patient_risk_profile
[2024-11-18T12:37:45.330+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:37:45.330+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:45.331+0000] {subprocess.py:106} INFO -     risk_id,
[2024-11-18T12:37:45.332+0000] {subprocess.py:106} INFO -     patient_risk_profile,
[2024-11-18T12:37:45.332+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:37:45.333+0000] {subprocess.py:106} INFO - from dim_risk
[2024-11-18T12:37:45.333+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:37:45.334+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.366+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: SELECT 2 in 0.048 seconds
[2024-11-18T12:37:45.381+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.383+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:37:45.384+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_risk" rename to "dim_risk__dbt_backup"
[2024-11-18T12:37:45.387+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.002 seconds
[2024-11-18T12:37:45.404+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.406+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:37:45.407+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_risk__dbt_tmp" rename to "dim_risk"
[2024-11-18T12:37:45.408+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:45.418+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: COMMIT
[2024-11-18T12:37:45.420+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.422+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: COMMIT
[2024-11-18T12:37:45.427+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: COMMIT in 0.003 seconds
[2024-11-18T12:37:45.438+0000] {subprocess.py:106} INFO - [0m12:37:45  Applying DROP to: "db"."public_analytics_schema"."dim_risk__dbt_backup"
[2024-11-18T12:37:45.441+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:37:45.443+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:37:45.445+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_risk__dbt_backup" cascade
[2024-11-18T12:37:45.453+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: DROP TABLE in 0.007 seconds
[2024-11-18T12:37:45.459+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.dim_risk: Close
[2024-11-18T12:37:45.462+0000] {subprocess.py:106} INFO - [0m12:37:45  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb916a06cf0>]}
[2024-11-18T12:37:45.466+0000] {subprocess.py:106} INFO - [0m12:37:45  4 of 6 OK created sql table model public_analytics_schema.dim_risk ............. [[32mSELECT 2[0m in 0.24s]
[2024-11-18T12:37:45.469+0000] {subprocess.py:106} INFO - [0m12:37:45  Finished running node model.etl_pipeline.dim_risk
[2024-11-18T12:37:45.471+0000] {subprocess.py:106} INFO - [0m12:37:45  Began running node model.etl_pipeline.fact_revenue
[2024-11-18T12:37:45.473+0000] {subprocess.py:106} INFO - [0m12:37:45  5 of 6 START sql table model public_analytics_schema.fact_revenue .............. [RUN]
[2024-11-18T12:37:45.476+0000] {subprocess.py:106} INFO - [0m12:37:45  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_risk, now model.etl_pipeline.fact_revenue)
[2024-11-18T12:37:45.478+0000] {subprocess.py:106} INFO - [0m12:37:45  Began compiling node model.etl_pipeline.fact_revenue
[2024-11-18T12:37:45.507+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing injected SQL for node "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.509+0000] {subprocess.py:106} INFO - [0m12:37:45  Began executing node model.etl_pipeline.fact_revenue
[2024-11-18T12:37:45.526+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing runtime sql for node "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.528+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.530+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: BEGIN
[2024-11-18T12:37:45.533+0000] {subprocess.py:106} INFO - [0m12:37:45  Opening a new connection, currently in state closed
[2024-11-18T12:37:45.563+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: BEGIN in 0.030 seconds
[2024-11-18T12:37:45.565+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.567+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:37:45.568+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.569+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.569+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.570+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.570+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."fact_revenue__dbt_tmp"
[2024-11-18T12:37:45.571+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.571+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.572+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:37:45.573+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.573+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:37:45.574+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.575+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.576+0000] {subprocess.py:106} INFO - -- Create the fact table for patient visits
[2024-11-18T12:37:45.577+0000] {subprocess.py:106} INFO - with fact_patient_visits as (
[2024-11-18T12:37:45.577+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:37:45.578+0000] {subprocess.py:106} INFO -         -- Surrogate keys from dimension tables
[2024-11-18T12:37:45.579+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(doctor as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as doctor_id,
[2024-11-18T12:37:45.580+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as branch_id,
[2024-11-18T12:37:45.581+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(patient_risk_profile as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as risk_id,
[2024-11-18T12:37:45.581+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.582+0000] {subprocess.py:106} INFO -         -- Natural keys and attributes
[2024-11-18T12:37:45.584+0000] {subprocess.py:106} INFO -         row_id,
[2024-11-18T12:37:45.585+0000] {subprocess.py:106} INFO -         patient_id,
[2024-11-18T12:37:45.586+0000] {subprocess.py:106} INFO -         department,
[2024-11-18T12:37:45.587+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.588+0000] {subprocess.py:106} INFO -         -- Metrics (facts)
[2024-11-18T12:37:45.588+0000] {subprocess.py:106} INFO -         revenue,
[2024-11-18T12:37:45.589+0000] {subprocess.py:106} INFO -         minutes_to_service,
[2024-11-18T12:37:45.589+0000] {subprocess.py:106} INFO -         number_of_patient_visits
[2024-11-18T12:37:45.590+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.590+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.591+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:37:45.591+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:37:45.592+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.593+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:37:45.594+0000] {subprocess.py:106} INFO -     -- Surrogate keys
[2024-11-18T12:37:45.595+0000] {subprocess.py:106} INFO -     doctor_id,
[2024-11-18T12:37:45.596+0000] {subprocess.py:106} INFO -     branch_id,
[2024-11-18T12:37:45.597+0000] {subprocess.py:106} INFO -     risk_id,
[2024-11-18T12:37:45.597+0000] {subprocess.py:106} INFO -     -- Natural keys
[2024-11-18T12:37:45.598+0000] {subprocess.py:106} INFO -     row_id,
[2024-11-18T12:37:45.599+0000] {subprocess.py:106} INFO -     patient_id,
[2024-11-18T12:37:45.599+0000] {subprocess.py:106} INFO -     -- Metrics
[2024-11-18T12:37:45.600+0000] {subprocess.py:106} INFO -     minutes_to_service,
[2024-11-18T12:37:45.601+0000] {subprocess.py:106} INFO -     number_of_patient_visits,
[2024-11-18T12:37:45.602+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:37:45.602+0000] {subprocess.py:106} INFO - from fact_patient_visits
[2024-11-18T12:37:45.603+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:37:45.604+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.683+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: SELECT 6334 in 0.114 seconds
[2024-11-18T12:37:45.695+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.698+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:37:45.698+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."fact_revenue" rename to "fact_revenue__dbt_backup"
[2024-11-18T12:37:45.700+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:45.713+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.715+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:37:45.717+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."fact_revenue__dbt_tmp" rename to "fact_revenue"
[2024-11-18T12:37:45.719+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.002 seconds
[2024-11-18T12:37:45.728+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: COMMIT
[2024-11-18T12:37:45.729+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.731+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: COMMIT
[2024-11-18T12:37:45.752+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: COMMIT in 0.019 seconds
[2024-11-18T12:37:45.764+0000] {subprocess.py:106} INFO - [0m12:37:45  Applying DROP to: "db"."public_analytics_schema"."fact_revenue__dbt_backup"
[2024-11-18T12:37:45.768+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:37:45.770+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:37:45.771+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."fact_revenue__dbt_backup" cascade
[2024-11-18T12:37:45.779+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: DROP TABLE in 0.007 seconds
[2024-11-18T12:37:45.786+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.fact_revenue: Close
[2024-11-18T12:37:45.788+0000] {subprocess.py:106} INFO - [0m12:37:45  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb91756b290>]}
[2024-11-18T12:37:45.791+0000] {subprocess.py:106} INFO - [0m12:37:45  5 of 6 OK created sql table model public_analytics_schema.fact_revenue ......... [[32mSELECT 6334[0m in 0.31s]
[2024-11-18T12:37:45.794+0000] {subprocess.py:106} INFO - [0m12:37:45  Finished running node model.etl_pipeline.fact_revenue
[2024-11-18T12:37:45.796+0000] {subprocess.py:106} INFO - [0m12:37:45  Began running node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:37:45.799+0000] {subprocess.py:106} INFO - [0m12:37:45  6 of 6 START sql table model public.staging_patient_visits ..................... [RUN]
[2024-11-18T12:37:45.801+0000] {subprocess.py:106} INFO - [0m12:37:45  Re-using an available connection from the pool (formerly model.etl_pipeline.fact_revenue, now model.etl_pipeline.staging_patient_visits)
[2024-11-18T12:37:45.802+0000] {subprocess.py:106} INFO - [0m12:37:45  Began compiling node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:37:45.816+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing injected SQL for node "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.819+0000] {subprocess.py:106} INFO - [0m12:37:45  Began executing node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:37:45.839+0000] {subprocess.py:106} INFO - [0m12:37:45  Writing runtime sql for node "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.842+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.844+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.staging_patient_visits: BEGIN
[2024-11-18T12:37:45.846+0000] {subprocess.py:106} INFO - [0m12:37:45  Opening a new connection, currently in state closed
[2024-11-18T12:37:45.874+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: BEGIN in 0.029 seconds
[2024-11-18T12:37:45.877+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.879+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:37:45.880+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.881+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.881+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.883+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.884+0000] {subprocess.py:106} INFO -   create  table "db"."public"."staging_patient_visits__dbt_tmp"
[2024-11-18T12:37:45.887+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.888+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.889+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:37:45.890+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.891+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:37:45.892+0000] {subprocess.py:106} INFO -     -- having staging_patient as table as it is temporary and has no updates
[2024-11-18T12:37:45.892+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.893+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.894+0000] {subprocess.py:106} INFO - -- loading the data extracted from the csv file
[2024-11-18T12:37:45.894+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.895+0000] {subprocess.py:106} INFO - SELECT *
[2024-11-18T12:37:45.896+0000] {subprocess.py:106} INFO - FROM "db"."public"."staging_patient_visits"
[2024-11-18T12:37:45.896+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:37:45.897+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:37:45.913+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: SELECT 6334 in 0.032 seconds
[2024-11-18T12:37:45.932+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.934+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:37:45.935+0000] {subprocess.py:106} INFO - alter table "db"."public"."staging_patient_visits" rename to "staging_patient_visits__dbt_backup"
[2024-11-18T12:37:45.938+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:45.951+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.953+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:37:45.954+0000] {subprocess.py:106} INFO - alter table "db"."public"."staging_patient_visits__dbt_tmp" rename to "staging_patient_visits"
[2024-11-18T12:37:45.956+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:37:45.965+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.staging_patient_visits: COMMIT
[2024-11-18T12:37:45.967+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:45.969+0000] {subprocess.py:106} INFO - [0m12:37:45  On model.etl_pipeline.staging_patient_visits: COMMIT
[2024-11-18T12:37:45.984+0000] {subprocess.py:106} INFO - [0m12:37:45  SQL status: COMMIT in 0.013 seconds
[2024-11-18T12:37:45.994+0000] {subprocess.py:106} INFO - [0m12:37:45  Applying DROP to: "db"."public"."staging_patient_visits__dbt_backup"
[2024-11-18T12:37:45.999+0000] {subprocess.py:106} INFO - [0m12:37:45  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:37:46.000+0000] {subprocess.py:106} INFO - [0m12:37:46  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:37:46.001+0000] {subprocess.py:106} INFO - drop table if exists "db"."public"."staging_patient_visits__dbt_backup" cascade
[2024-11-18T12:37:46.008+0000] {subprocess.py:106} INFO - [0m12:37:46  SQL status: DROP TABLE in 0.006 seconds
[2024-11-18T12:37:46.015+0000] {subprocess.py:106} INFO - [0m12:37:46  On model.etl_pipeline.staging_patient_visits: Close
[2024-11-18T12:37:46.019+0000] {subprocess.py:106} INFO - [0m12:37:46  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82b53b71-7737-47ba-9ebc-1b50e2299163', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb916b94bc0>]}
[2024-11-18T12:37:46.021+0000] {subprocess.py:106} INFO - [0m12:37:46  6 of 6 OK created sql table model public.staging_patient_visits ................ [[32mSELECT 6334[0m in 0.22s]
[2024-11-18T12:37:46.024+0000] {subprocess.py:106} INFO - [0m12:37:46  Finished running node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:37:46.029+0000] {subprocess.py:106} INFO - [0m12:37:46  Using postgres connection "master"
[2024-11-18T12:37:46.030+0000] {subprocess.py:106} INFO - [0m12:37:46  On master: BEGIN
[2024-11-18T12:37:46.032+0000] {subprocess.py:106} INFO - [0m12:37:46  Opening a new connection, currently in state closed
[2024-11-18T12:37:46.060+0000] {subprocess.py:106} INFO - [0m12:37:46  SQL status: BEGIN in 0.028 seconds
[2024-11-18T12:37:46.063+0000] {subprocess.py:106} INFO - [0m12:37:46  On master: COMMIT
[2024-11-18T12:37:46.065+0000] {subprocess.py:106} INFO - [0m12:37:46  Using postgres connection "master"
[2024-11-18T12:37:46.067+0000] {subprocess.py:106} INFO - [0m12:37:46  On master: COMMIT
[2024-11-18T12:37:46.069+0000] {subprocess.py:106} INFO - [0m12:37:46  SQL status: COMMIT in 0.000 seconds
[2024-11-18T12:37:46.070+0000] {subprocess.py:106} INFO - [0m12:37:46  On master: Close
[2024-11-18T12:37:46.073+0000] {subprocess.py:106} INFO - [0m12:37:46  Connection 'master' was properly closed.
[2024-11-18T12:37:46.074+0000] {subprocess.py:106} INFO - [0m12:37:46  Connection 'model.etl_pipeline.staging_patient_visits' was properly closed.
[2024-11-18T12:37:46.076+0000] {subprocess.py:106} INFO - [0m12:37:46
[2024-11-18T12:37:46.078+0000] {subprocess.py:106} INFO - [0m12:37:46  Finished running 6 table models in 0 hours 0 minutes and 2.06 seconds (2.06s).
[2024-11-18T12:37:46.085+0000] {subprocess.py:106} INFO - [0m12:37:46  Command end result
[2024-11-18T12:37:46.398+0000] {subprocess.py:106} INFO - [0m12:37:46
[2024-11-18T12:37:46.400+0000] {subprocess.py:106} INFO - [0m12:37:46  [32mCompleted successfully[0m
[2024-11-18T12:37:46.402+0000] {subprocess.py:106} INFO - [0m12:37:46
[2024-11-18T12:37:46.404+0000] {subprocess.py:106} INFO - [0m12:37:46  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[2024-11-18T12:37:46.408+0000] {subprocess.py:106} INFO - [0m12:37:46  Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.0295606, "process_user_time": 5.119448, "process_kernel_time": 0.396647, "process_mem_max_rss": "116800", "process_in_blocks": "12264", "process_out_blocks": "2800"}
[2024-11-18T12:37:46.411+0000] {subprocess.py:106} INFO - [0m12:37:46  Command `dbt run` succeeded at 12:37:46.408983 after 5.03 seconds
[2024-11-18T12:37:46.413+0000] {subprocess.py:106} INFO - [0m12:37:46  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9191dc9b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb91a9ffd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb916fb37a0>]}
[2024-11-18T12:37:46.415+0000] {subprocess.py:106} INFO - [0m12:37:46  Flushing usage events
[2024-11-18T12:37:48.126+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2024-11-18T12:37:48.156+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-18T12:37:48.156+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=etl_pipeline_dag, task_id=dbt_run, run_id=scheduled__2024-11-17T00:00:00+00:00, execution_date=20241117T000000, start_date=20241118T123739, end_date=20241118T123748
[2024-11-18T12:37:48.215+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-11-18T12:37:48.266+0000] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-18T12:37:48.268+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
