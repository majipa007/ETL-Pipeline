[2024-11-18T12:40:11.683+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-18T12:40:11.709+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_pipeline_dag.dbt_run manual__2024-11-18T12:39:53.182771+00:00 [queued]>
[2024-11-18T12:40:11.727+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_pipeline_dag.dbt_run manual__2024-11-18T12:39:53.182771+00:00 [queued]>
[2024-11-18T12:40:11.728+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2024-11-18T12:40:11.768+0000] {taskinstance.py:2889} INFO - Executing <Task(BashOperator): dbt_run> on 2024-11-18 12:39:53.182771+00:00
[2024-11-18T12:40:11.779+0000] {standard_task_runner.py:72} INFO - Started process 254 to run task
[2024-11-18T12:40:11.784+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'etl_pipeline_dag', 'dbt_run', 'manual__2024-11-18T12:39:53.182771+00:00', '--job-id', '22', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp93dws49e']
[2024-11-18T12:40:11.788+0000] {standard_task_runner.py:105} INFO - Job 22: Subtask dbt_run
[2024-11-18T12:40:11.867+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_pipeline_dag.dbt_run manual__2024-11-18T12:39:53.182771+00:00 [running]> on host 945969587f65
[2024-11-18T12:40:12.027+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='dbt_run' AIRFLOW_CTX_EXECUTION_DATE='2024-11-18T12:39:53.182771+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-18T12:39:53.182771+00:00'
[2024-11-18T12:40:12.029+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-18T12:40:12.079+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2024-11-18T12:40:12.081+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', '\n            cd /opt/airflow/etl_pipeline &&             echo "Running dbt with profiles.yml in: $DBT_PROFILES_DIR" &&             dbt run --debug --profiles-dir /opt/airflow/etl_pipeline\n        ']
[2024-11-18T12:40:12.099+0000] {subprocess.py:99} INFO - Output:
[2024-11-18T12:40:12.103+0000] {subprocess.py:106} INFO - Running dbt with profiles.yml in:
[2024-11-18T12:40:13.701+0000] {subprocess.py:106} INFO - [0m12:40:13  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a09cfc710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a0825d310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a081ba930>]}
[2024-11-18T12:40:13.702+0000] {subprocess.py:106} INFO - [0m12:40:13  Running with dbt=1.8.8
[2024-11-18T12:40:13.702+0000] {subprocess.py:106} INFO - [0m12:40:13  running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/opt/airflow/etl_pipeline', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/opt/airflow/etl_pipeline/logs', 'debug': 'True', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --debug --profiles-dir /opt/airflow/etl_pipeline', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[2024-11-18T12:40:13.916+0000] {subprocess.py:106} INFO - [0m12:40:13  Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a0783f950>]}
[2024-11-18T12:40:13.991+0000] {subprocess.py:106} INFO - [0m12:40:13  Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a07d87950>]}
[2024-11-18T12:40:13.992+0000] {subprocess.py:106} INFO - [0m12:40:13  Registered adapter: postgres=1.8.2
[2024-11-18T12:40:14.014+0000] {subprocess.py:106} INFO - [0m12:40:14  checksum: ec53e341a936509427e16df1c646386a84befec52c1957e3f4359fbc36fc93f5, vars: {}, profile: , target: , version: 1.8.8
[2024-11-18T12:40:14.178+0000] {subprocess.py:106} INFO - [0m12:40:14  Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[2024-11-18T12:40:14.180+0000] {subprocess.py:106} INFO - [0m12:40:14  Partial parsing: updated file: etl_pipeline://models/transformation/schema.yml
[2024-11-18T12:40:14.528+0000] {subprocess.py:106} INFO - [0m12:40:14  [[33mWARNING[0m]: Deprecated functionality
[2024-11-18T12:40:14.529+0000] {subprocess.py:106} INFO - The `tests` config has been renamed to `data_tests`. Please see
[2024-11-18T12:40:14.529+0000] {subprocess.py:106} INFO - https://docs.getdbt.com/docs/build/data-tests#new-data_tests-syntax for more
[2024-11-18T12:40:14.529+0000] {subprocess.py:106} INFO - information.
[2024-11-18T12:40:14.530+0000] {subprocess.py:106} INFO - [0m12:40:14  Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a07175c40>]}
[2024-11-18T12:40:14.788+0000] {subprocess.py:106} INFO - [0m12:40:14  [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
[2024-11-18T12:40:14.789+0000] {subprocess.py:106} INFO - There are 1 unused configuration paths:
[2024-11-18T12:40:14.789+0000] {subprocess.py:106} INFO - - models.etl_pipeline.production
[2024-11-18T12:40:14.802+0000] {subprocess.py:106} INFO - [0m12:40:14  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a06c16a50>]}
[2024-11-18T12:40:14.908+0000] {subprocess.py:106} INFO - [0m12:40:14  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a06db5700>]}
[2024-11-18T12:40:14.909+0000] {subprocess.py:106} INFO - [0m12:40:14  Found 6 models, 25 data tests, 2 sources, 539 macros
[2024-11-18T12:40:14.909+0000] {subprocess.py:106} INFO - [0m12:40:14  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a071b68d0>]}
[2024-11-18T12:40:14.912+0000] {subprocess.py:106} INFO - [0m12:40:14
[2024-11-18T12:40:14.912+0000] {subprocess.py:106} INFO - [0m12:40:14  Acquiring new postgres connection 'master'
[2024-11-18T12:40:14.916+0000] {subprocess.py:106} INFO - [0m12:40:14  Acquiring new postgres connection 'list_db'
[2024-11-18T12:40:14.949+0000] {subprocess.py:106} INFO - [0m12:40:14  Using postgres connection "list_db"
[2024-11-18T12:40:14.950+0000] {subprocess.py:106} INFO - [0m12:40:14  On list_db: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db"} */
[2024-11-18T12:40:14.950+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:14.950+0000] {subprocess.py:106} INFO -     select distinct nspname from pg_namespace
[2024-11-18T12:40:14.950+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:14.951+0000] {subprocess.py:106} INFO - [0m12:40:14  Opening a new connection, currently in state init
[2024-11-18T12:40:14.960+0000] {subprocess.py:106} INFO - [0m12:40:14  SQL status: SELECT 6 in 0.009 seconds
[2024-11-18T12:40:14.961+0000] {subprocess.py:106} INFO - [0m12:40:14  On list_db: Close
[2024-11-18T12:40:14.964+0000] {subprocess.py:106} INFO - [0m12:40:14  Using postgres connection "list_db"
[2024-11-18T12:40:14.964+0000] {subprocess.py:106} INFO - [0m12:40:14  On list_db: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db"} */
[2024-11-18T12:40:14.965+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:14.965+0000] {subprocess.py:106} INFO -     select distinct nspname from pg_namespace
[2024-11-18T12:40:14.965+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:14.965+0000] {subprocess.py:106} INFO - [0m12:40:14  Opening a new connection, currently in state closed
[2024-11-18T12:40:14.974+0000] {subprocess.py:106} INFO - [0m12:40:14  SQL status: SELECT 6 in 0.009 seconds
[2024-11-18T12:40:14.976+0000] {subprocess.py:106} INFO - [0m12:40:14  On list_db: Close
[2024-11-18T12:40:14.978+0000] {subprocess.py:106} INFO - [0m12:40:14  Re-using an available connection from the pool (formerly list_db, now list_db_public_analytics_schema)
[2024-11-18T12:40:14.987+0000] {subprocess.py:106} INFO - [0m12:40:14  Using postgres connection "list_db_public_analytics_schema"
[2024-11-18T12:40:14.987+0000] {subprocess.py:106} INFO - [0m12:40:14  On list_db_public_analytics_schema: BEGIN
[2024-11-18T12:40:14.988+0000] {subprocess.py:106} INFO - [0m12:40:14  Opening a new connection, currently in state closed
[2024-11-18T12:40:15.000+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: BEGIN in 0.012 seconds
[2024-11-18T12:40:15.001+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "list_db_public_analytics_schema"
[2024-11-18T12:40:15.002+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public_analytics_schema: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db_public_analytics_schema"} */
[2024-11-18T12:40:15.003+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:15.003+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:40:15.004+0000] {subprocess.py:106} INFO -       tablename as name,
[2024-11-18T12:40:15.004+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:40:15.005+0000] {subprocess.py:106} INFO -       'table' as type
[2024-11-18T12:40:15.005+0000] {subprocess.py:106} INFO -     from pg_tables
[2024-11-18T12:40:15.006+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:40:15.006+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:40:15.007+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.007+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:40:15.008+0000] {subprocess.py:106} INFO -       viewname as name,
[2024-11-18T12:40:15.009+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:40:15.009+0000] {subprocess.py:106} INFO -       'view' as type
[2024-11-18T12:40:15.010+0000] {subprocess.py:106} INFO -     from pg_views
[2024-11-18T12:40:15.010+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:40:15.011+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:40:15.011+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.012+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:40:15.012+0000] {subprocess.py:106} INFO -       matviewname as name,
[2024-11-18T12:40:15.012+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:40:15.013+0000] {subprocess.py:106} INFO -       'materialized_view' as type
[2024-11-18T12:40:15.013+0000] {subprocess.py:106} INFO -     from pg_matviews
[2024-11-18T12:40:15.013+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:40:15.014+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.014+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: SELECT 5 in 0.008 seconds
[2024-11-18T12:40:15.015+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public_analytics_schema: ROLLBACK
[2024-11-18T12:40:15.016+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public_analytics_schema: Close
[2024-11-18T12:40:15.018+0000] {subprocess.py:106} INFO - [0m12:40:15  Re-using an available connection from the pool (formerly list_db_public_analytics_schema, now list_db_public)
[2024-11-18T12:40:15.027+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "list_db_public"
[2024-11-18T12:40:15.029+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public: BEGIN
[2024-11-18T12:40:15.030+0000] {subprocess.py:106} INFO - [0m12:40:15  Opening a new connection, currently in state closed
[2024-11-18T12:40:15.051+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: BEGIN in 0.021 seconds
[2024-11-18T12:40:15.053+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "list_db_public"
[2024-11-18T12:40:15.054+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db_public"} */
[2024-11-18T12:40:15.055+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:15.055+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:40:15.056+0000] {subprocess.py:106} INFO -       tablename as name,
[2024-11-18T12:40:15.057+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:40:15.058+0000] {subprocess.py:106} INFO -       'table' as type
[2024-11-18T12:40:15.059+0000] {subprocess.py:106} INFO -     from pg_tables
[2024-11-18T12:40:15.059+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:40:15.060+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:40:15.061+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.062+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:40:15.063+0000] {subprocess.py:106} INFO -       viewname as name,
[2024-11-18T12:40:15.063+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:40:15.064+0000] {subprocess.py:106} INFO -       'view' as type
[2024-11-18T12:40:15.065+0000] {subprocess.py:106} INFO -     from pg_views
[2024-11-18T12:40:15.066+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:40:15.066+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:40:15.067+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.068+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:40:15.068+0000] {subprocess.py:106} INFO -       matviewname as name,
[2024-11-18T12:40:15.069+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:40:15.069+0000] {subprocess.py:106} INFO -       'materialized_view' as type
[2024-11-18T12:40:15.070+0000] {subprocess.py:106} INFO -     from pg_matviews
[2024-11-18T12:40:15.070+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:40:15.071+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.071+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: SELECT 1 in 0.011 seconds
[2024-11-18T12:40:15.072+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public: ROLLBACK
[2024-11-18T12:40:15.073+0000] {subprocess.py:106} INFO - [0m12:40:15  On list_db_public: Close
[2024-11-18T12:40:15.095+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "master"
[2024-11-18T12:40:15.097+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: BEGIN
[2024-11-18T12:40:15.099+0000] {subprocess.py:106} INFO - [0m12:40:15  Opening a new connection, currently in state init
[2024-11-18T12:40:15.127+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: BEGIN in 0.027 seconds
[2024-11-18T12:40:15.131+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "master"
[2024-11-18T12:40:15.136+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "master"} */
[2024-11-18T12:40:15.137+0000] {subprocess.py:106} INFO - with relation as (
[2024-11-18T12:40:15.138+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:40:15.139+0000] {subprocess.py:106} INFO -             pg_rewrite.ev_class as class,
[2024-11-18T12:40:15.140+0000] {subprocess.py:106} INFO -             pg_rewrite.oid as id
[2024-11-18T12:40:15.141+0000] {subprocess.py:106} INFO -         from pg_rewrite
[2024-11-18T12:40:15.141+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:40:15.142+0000] {subprocess.py:106} INFO -     class as (
[2024-11-18T12:40:15.143+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:40:15.143+0000] {subprocess.py:106} INFO -             oid as id,
[2024-11-18T12:40:15.144+0000] {subprocess.py:106} INFO -             relname as name,
[2024-11-18T12:40:15.145+0000] {subprocess.py:106} INFO -             relnamespace as schema,
[2024-11-18T12:40:15.146+0000] {subprocess.py:106} INFO -             relkind as kind
[2024-11-18T12:40:15.147+0000] {subprocess.py:106} INFO -         from pg_class
[2024-11-18T12:40:15.148+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:40:15.149+0000] {subprocess.py:106} INFO -     dependency as (
[2024-11-18T12:40:15.150+0000] {subprocess.py:106} INFO -         select distinct
[2024-11-18T12:40:15.151+0000] {subprocess.py:106} INFO -             pg_depend.objid as id,
[2024-11-18T12:40:15.151+0000] {subprocess.py:106} INFO -             pg_depend.refobjid as ref
[2024-11-18T12:40:15.152+0000] {subprocess.py:106} INFO -         from pg_depend
[2024-11-18T12:40:15.152+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:40:15.153+0000] {subprocess.py:106} INFO -     schema as (
[2024-11-18T12:40:15.153+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:40:15.154+0000] {subprocess.py:106} INFO -             pg_namespace.oid as id,
[2024-11-18T12:40:15.155+0000] {subprocess.py:106} INFO -             pg_namespace.nspname as name
[2024-11-18T12:40:15.155+0000] {subprocess.py:106} INFO -         from pg_namespace
[2024-11-18T12:40:15.156+0000] {subprocess.py:106} INFO -         where nspname != 'information_schema' and nspname not like 'pg\_%'
[2024-11-18T12:40:15.156+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:40:15.157+0000] {subprocess.py:106} INFO -     referenced as (
[2024-11-18T12:40:15.158+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:40:15.159+0000] {subprocess.py:106} INFO -             relation.id AS id,
[2024-11-18T12:40:15.160+0000] {subprocess.py:106} INFO -             referenced_class.name ,
[2024-11-18T12:40:15.160+0000] {subprocess.py:106} INFO -             referenced_class.schema ,
[2024-11-18T12:40:15.161+0000] {subprocess.py:106} INFO -             referenced_class.kind
[2024-11-18T12:40:15.161+0000] {subprocess.py:106} INFO -         from relation
[2024-11-18T12:40:15.162+0000] {subprocess.py:106} INFO -         join class as referenced_class on relation.class=referenced_class.id
[2024-11-18T12:40:15.162+0000] {subprocess.py:106} INFO -         where referenced_class.kind in ('r', 'v', 'm')
[2024-11-18T12:40:15.163+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:40:15.164+0000] {subprocess.py:106} INFO -     relationships as (
[2024-11-18T12:40:15.165+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:40:15.166+0000] {subprocess.py:106} INFO -             referenced.name as referenced_name,
[2024-11-18T12:40:15.166+0000] {subprocess.py:106} INFO -             referenced.schema as referenced_schema_id,
[2024-11-18T12:40:15.167+0000] {subprocess.py:106} INFO -             dependent_class.name as dependent_name,
[2024-11-18T12:40:15.167+0000] {subprocess.py:106} INFO -             dependent_class.schema as dependent_schema_id,
[2024-11-18T12:40:15.168+0000] {subprocess.py:106} INFO -             referenced.kind as kind
[2024-11-18T12:40:15.169+0000] {subprocess.py:106} INFO -         from referenced
[2024-11-18T12:40:15.169+0000] {subprocess.py:106} INFO -         join dependency on referenced.id=dependency.id
[2024-11-18T12:40:15.170+0000] {subprocess.py:106} INFO -         join class as dependent_class on dependency.ref=dependent_class.id
[2024-11-18T12:40:15.170+0000] {subprocess.py:106} INFO -         where
[2024-11-18T12:40:15.171+0000] {subprocess.py:106} INFO -             (referenced.name != dependent_class.name or
[2024-11-18T12:40:15.171+0000] {subprocess.py:106} INFO -              referenced.schema != dependent_class.schema)
[2024-11-18T12:40:15.172+0000] {subprocess.py:106} INFO -     )
[2024-11-18T12:40:15.173+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.173+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.174+0000] {subprocess.py:106} INFO -         referenced_schema.name as referenced_schema,
[2024-11-18T12:40:15.174+0000] {subprocess.py:106} INFO -         relationships.referenced_name as referenced_name,
[2024-11-18T12:40:15.175+0000] {subprocess.py:106} INFO -         dependent_schema.name as dependent_schema,
[2024-11-18T12:40:15.175+0000] {subprocess.py:106} INFO -         relationships.dependent_name as dependent_name
[2024-11-18T12:40:15.176+0000] {subprocess.py:106} INFO -     from relationships
[2024-11-18T12:40:15.176+0000] {subprocess.py:106} INFO -     join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
[2024-11-18T12:40:15.177+0000] {subprocess.py:106} INFO -     join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
[2024-11-18T12:40:15.178+0000] {subprocess.py:106} INFO -     group by referenced_schema, referenced_name, dependent_schema, dependent_name
[2024-11-18T12:40:15.180+0000] {subprocess.py:106} INFO -     order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[2024-11-18T12:40:15.181+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: SELECT 0 in 0.017 seconds
[2024-11-18T12:40:15.182+0000] {subprocess.py:106} INFO - [0m12:40:15  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a06d89c70>]}
[2024-11-18T12:40:15.183+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: ROLLBACK
[2024-11-18T12:40:15.183+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "master"
[2024-11-18T12:40:15.184+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: BEGIN
[2024-11-18T12:40:15.185+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: BEGIN in 0.001 seconds
[2024-11-18T12:40:15.185+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: COMMIT
[2024-11-18T12:40:15.186+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "master"
[2024-11-18T12:40:15.187+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: COMMIT
[2024-11-18T12:40:15.187+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: COMMIT in 0.000 seconds
[2024-11-18T12:40:15.188+0000] {subprocess.py:106} INFO - [0m12:40:15  On master: Close
[2024-11-18T12:40:15.189+0000] {subprocess.py:106} INFO - [0m12:40:15  Concurrency: 1 threads (target='dev')
[2024-11-18T12:40:15.189+0000] {subprocess.py:106} INFO - [0m12:40:15
[2024-11-18T12:40:15.196+0000] {subprocess.py:106} INFO - [0m12:40:15  Began running node model.etl_pipeline.dim_branches
[2024-11-18T12:40:15.198+0000] {subprocess.py:106} INFO - [0m12:40:15  1 of 6 START sql table model public_analytics_schema.dim_branches .............. [RUN]
[2024-11-18T12:40:15.200+0000] {subprocess.py:106} INFO - [0m12:40:15  Re-using an available connection from the pool (formerly list_db_public, now model.etl_pipeline.dim_branches)
[2024-11-18T12:40:15.201+0000] {subprocess.py:106} INFO - [0m12:40:15  Began compiling node model.etl_pipeline.dim_branches
[2024-11-18T12:40:15.245+0000] {subprocess.py:106} INFO - [0m12:40:15  Writing injected SQL for node "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.248+0000] {subprocess.py:106} INFO - [0m12:40:15  Began executing node model.etl_pipeline.dim_branches
[2024-11-18T12:40:15.415+0000] {subprocess.py:106} INFO - [0m12:40:15  Writing runtime sql for node "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.417+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.418+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: BEGIN
[2024-11-18T12:40:15.419+0000] {subprocess.py:106} INFO - [0m12:40:15  Opening a new connection, currently in state closed
[2024-11-18T12:40:15.433+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: BEGIN in 0.013 seconds
[2024-11-18T12:40:15.434+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.435+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:40:15.435+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.435+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.436+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.436+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.436+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_branches__dbt_tmp"
[2024-11-18T12:40:15.436+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.437+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.437+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:40:15.437+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.437+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:40:15.438+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.438+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.438+0000] {subprocess.py:106} INFO - -- Create the doctors dimension table in the analytics_schema
[2024-11-18T12:40:15.439+0000] {subprocess.py:106} INFO - with dim_bra as (
[2024-11-18T12:40:15.439+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.439+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as branch_id,
[2024-11-18T12:40:15.439+0000] {subprocess.py:106} INFO -         hospital_branch,
[2024-11-18T12:40:15.440+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:40:15.440+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:40:15.440+0000] {subprocess.py:106} INFO -     group by branch_id, hospital_branch
[2024-11-18T12:40:15.440+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:40:15.441+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:15.441+0000] {subprocess.py:106} INFO -     branch_id,
[2024-11-18T12:40:15.441+0000] {subprocess.py:106} INFO -     hospital_branch,
[2024-11-18T12:40:15.442+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:40:15.442+0000] {subprocess.py:106} INFO - from dim_bra
[2024-11-18T12:40:15.442+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:40:15.442+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.486+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: SELECT 3 in 0.050 seconds
[2024-11-18T12:40:15.626+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.628+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:40:15.628+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_branches" rename to "dim_branches__dbt_backup"
[2024-11-18T12:40:15.630+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:15.641+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.643+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:40:15.643+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_branches__dbt_tmp" rename to "dim_branches"
[2024-11-18T12:40:15.645+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:15.714+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: COMMIT
[2024-11-18T12:40:15.715+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.716+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: COMMIT
[2024-11-18T12:40:15.723+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: COMMIT in 0.005 seconds
[2024-11-18T12:40:15.742+0000] {subprocess.py:106} INFO - [0m12:40:15  Applying DROP to: "db"."public_analytics_schema"."dim_branches__dbt_backup"
[2024-11-18T12:40:15.758+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:40:15.759+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:40:15.760+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_branches__dbt_backup" cascade
[2024-11-18T12:40:15.769+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: DROP TABLE in 0.008 seconds
[2024-11-18T12:40:15.776+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_branches: Close
[2024-11-18T12:40:15.781+0000] {subprocess.py:106} INFO - [0m12:40:15  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a06125f70>]}
[2024-11-18T12:40:15.783+0000] {subprocess.py:106} INFO - [0m12:40:15  1 of 6 OK created sql table model public_analytics_schema.dim_branches ......... [[32mSELECT 3[0m in 0.58s]
[2024-11-18T12:40:15.785+0000] {subprocess.py:106} INFO - [0m12:40:15  Finished running node model.etl_pipeline.dim_branches
[2024-11-18T12:40:15.787+0000] {subprocess.py:106} INFO - [0m12:40:15  Began running node model.etl_pipeline.dim_doctors
[2024-11-18T12:40:15.789+0000] {subprocess.py:106} INFO - [0m12:40:15  2 of 6 START sql table model public_analytics_schema.dim_doctors ............... [RUN]
[2024-11-18T12:40:15.790+0000] {subprocess.py:106} INFO - [0m12:40:15  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_branches, now model.etl_pipeline.dim_doctors)
[2024-11-18T12:40:15.792+0000] {subprocess.py:106} INFO - [0m12:40:15  Began compiling node model.etl_pipeline.dim_doctors
[2024-11-18T12:40:15.817+0000] {subprocess.py:106} INFO - [0m12:40:15  Writing injected SQL for node "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.821+0000] {subprocess.py:106} INFO - [0m12:40:15  Began executing node model.etl_pipeline.dim_doctors
[2024-11-18T12:40:15.839+0000] {subprocess.py:106} INFO - [0m12:40:15  Writing runtime sql for node "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.841+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.842+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_doctors: BEGIN
[2024-11-18T12:40:15.844+0000] {subprocess.py:106} INFO - [0m12:40:15  Opening a new connection, currently in state closed
[2024-11-18T12:40:15.869+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: BEGIN in 0.024 seconds
[2024-11-18T12:40:15.870+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.872+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:40:15.872+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.873+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.873+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.874+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.874+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_doctors__dbt_tmp"
[2024-11-18T12:40:15.875+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.875+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.876+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:40:15.877+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.877+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:40:15.878+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.879+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.879+0000] {subprocess.py:106} INFO - -- Create the doctors dimension table in the analytics_schema
[2024-11-18T12:40:15.880+0000] {subprocess.py:106} INFO - with dim_doc as (
[2024-11-18T12:40:15.880+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:15.881+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(doctor as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(department as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as doctor_id,
[2024-11-18T12:40:15.882+0000] {subprocess.py:106} INFO -         doctor,
[2024-11-18T12:40:15.882+0000] {subprocess.py:106} INFO -         department,
[2024-11-18T12:40:15.883+0000] {subprocess.py:106} INFO -         count(*) as total_visits,
[2024-11-18T12:40:15.883+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:40:15.884+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:40:15.884+0000] {subprocess.py:106} INFO -     group by doctor_id, doctor, department
[2024-11-18T12:40:15.885+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:40:15.885+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:15.886+0000] {subprocess.py:106} INFO -     doctor_id,
[2024-11-18T12:40:15.886+0000] {subprocess.py:106} INFO -     doctor,
[2024-11-18T12:40:15.887+0000] {subprocess.py:106} INFO -     department,
[2024-11-18T12:40:15.887+0000] {subprocess.py:106} INFO -     total_visits,
[2024-11-18T12:40:15.888+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:40:15.888+0000] {subprocess.py:106} INFO - from dim_doc
[2024-11-18T12:40:15.889+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:40:15.889+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:15.929+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: SELECT 31 in 0.056 seconds
[2024-11-18T12:40:15.942+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.944+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:40:15.945+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_doctors" rename to "dim_doctors__dbt_backup"
[2024-11-18T12:40:15.947+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:15.960+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.963+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:40:15.964+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_doctors__dbt_tmp" rename to "dim_doctors"
[2024-11-18T12:40:15.967+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:15.976+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_doctors: COMMIT
[2024-11-18T12:40:15.978+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:15.980+0000] {subprocess.py:106} INFO - [0m12:40:15  On model.etl_pipeline.dim_doctors: COMMIT
[2024-11-18T12:40:15.986+0000] {subprocess.py:106} INFO - [0m12:40:15  SQL status: COMMIT in 0.005 seconds
[2024-11-18T12:40:15.996+0000] {subprocess.py:106} INFO - [0m12:40:15  Applying DROP to: "db"."public_analytics_schema"."dim_doctors__dbt_backup"
[2024-11-18T12:40:15.999+0000] {subprocess.py:106} INFO - [0m12:40:15  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:40:16.000+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:40:16.001+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_doctors__dbt_backup" cascade
[2024-11-18T12:40:16.013+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: DROP TABLE in 0.010 seconds
[2024-11-18T12:40:16.018+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_doctors: Close
[2024-11-18T12:40:16.020+0000] {subprocess.py:106} INFO - [0m12:40:16  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a0719c8f0>]}
[2024-11-18T12:40:16.022+0000] {subprocess.py:106} INFO - [0m12:40:16  2 of 6 OK created sql table model public_analytics_schema.dim_doctors .......... [[32mSELECT 31[0m in 0.23s]
[2024-11-18T12:40:16.024+0000] {subprocess.py:106} INFO - [0m12:40:16  Finished running node model.etl_pipeline.dim_doctors
[2024-11-18T12:40:16.026+0000] {subprocess.py:106} INFO - [0m12:40:16  Began running node model.etl_pipeline.dim_patients
[2024-11-18T12:40:16.028+0000] {subprocess.py:106} INFO - [0m12:40:16  3 of 6 START sql table model public_analytics_schema.dim_patients .............. [RUN]
[2024-11-18T12:40:16.030+0000] {subprocess.py:106} INFO - [0m12:40:16  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_doctors, now model.etl_pipeline.dim_patients)
[2024-11-18T12:40:16.031+0000] {subprocess.py:106} INFO - [0m12:40:16  Began compiling node model.etl_pipeline.dim_patients
[2024-11-18T12:40:16.046+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing injected SQL for node "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.048+0000] {subprocess.py:106} INFO - [0m12:40:16  Began executing node model.etl_pipeline.dim_patients
[2024-11-18T12:40:16.059+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing runtime sql for node "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.061+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.063+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: BEGIN
[2024-11-18T12:40:16.064+0000] {subprocess.py:106} INFO - [0m12:40:16  Opening a new connection, currently in state closed
[2024-11-18T12:40:16.085+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: BEGIN in 0.020 seconds
[2024-11-18T12:40:16.086+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.087+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:40:16.088+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.088+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.089+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.090+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.090+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_patients__dbt_tmp"
[2024-11-18T12:40:16.091+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.091+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.092+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:40:16.092+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.093+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:40:16.093+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.094+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.094+0000] {subprocess.py:106} INFO - -- Create the patients dimension table in the analytics_schema
[2024-11-18T12:40:16.094+0000] {subprocess.py:106} INFO - with dim_pat as (
[2024-11-18T12:40:16.095+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:16.095+0000] {subprocess.py:106} INFO -         patient_id,
[2024-11-18T12:40:16.096+0000] {subprocess.py:106} INFO -         patient_name,
[2024-11-18T12:40:16.096+0000] {subprocess.py:106} INFO -         count(*) as total_visits,
[2024-11-18T12:40:16.097+0000] {subprocess.py:106} INFO -         sum(revenue) as total_spent
[2024-11-18T12:40:16.097+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:40:16.097+0000] {subprocess.py:106} INFO -     group by patient_id, patient_name
[2024-11-18T12:40:16.098+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:40:16.098+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:16.099+0000] {subprocess.py:106} INFO -     patient_id,
[2024-11-18T12:40:16.099+0000] {subprocess.py:106} INFO -     patient_name,
[2024-11-18T12:40:16.100+0000] {subprocess.py:106} INFO -     total_visits,
[2024-11-18T12:40:16.100+0000] {subprocess.py:106} INFO -     total_spent
[2024-11-18T12:40:16.100+0000] {subprocess.py:106} INFO - from dim_pat
[2024-11-18T12:40:16.101+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:40:16.101+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.109+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: SELECT 2423 in 0.020 seconds
[2024-11-18T12:40:16.119+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.121+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:40:16.121+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_patients" rename to "dim_patients__dbt_backup"
[2024-11-18T12:40:16.123+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.133+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.135+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:40:16.135+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_patients__dbt_tmp" rename to "dim_patients"
[2024-11-18T12:40:16.137+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.144+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: COMMIT
[2024-11-18T12:40:16.145+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.146+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: COMMIT
[2024-11-18T12:40:16.152+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: COMMIT in 0.004 seconds
[2024-11-18T12:40:16.160+0000] {subprocess.py:106} INFO - [0m12:40:16  Applying DROP to: "db"."public_analytics_schema"."dim_patients__dbt_backup"
[2024-11-18T12:40:16.163+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:40:16.165+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:40:16.165+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_patients__dbt_backup" cascade
[2024-11-18T12:40:16.171+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: DROP TABLE in 0.005 seconds
[2024-11-18T12:40:16.176+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_patients: Close
[2024-11-18T12:40:16.179+0000] {subprocess.py:106} INFO - [0m12:40:16  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a0783ffe0>]}
[2024-11-18T12:40:16.183+0000] {subprocess.py:106} INFO - [0m12:40:16  3 of 6 OK created sql table model public_analytics_schema.dim_patients ......... [[32mSELECT 2423[0m in 0.15s]
[2024-11-18T12:40:16.186+0000] {subprocess.py:106} INFO - [0m12:40:16  Finished running node model.etl_pipeline.dim_patients
[2024-11-18T12:40:16.188+0000] {subprocess.py:106} INFO - [0m12:40:16  Began running node model.etl_pipeline.dim_risk
[2024-11-18T12:40:16.190+0000] {subprocess.py:106} INFO - [0m12:40:16  4 of 6 START sql table model public_analytics_schema.dim_risk .................. [RUN]
[2024-11-18T12:40:16.191+0000] {subprocess.py:106} INFO - [0m12:40:16  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_patients, now model.etl_pipeline.dim_risk)
[2024-11-18T12:40:16.193+0000] {subprocess.py:106} INFO - [0m12:40:16  Began compiling node model.etl_pipeline.dim_risk
[2024-11-18T12:40:16.209+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing injected SQL for node "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.211+0000] {subprocess.py:106} INFO - [0m12:40:16  Began executing node model.etl_pipeline.dim_risk
[2024-11-18T12:40:16.228+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing runtime sql for node "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.230+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.232+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: BEGIN
[2024-11-18T12:40:16.235+0000] {subprocess.py:106} INFO - [0m12:40:16  Opening a new connection, currently in state closed
[2024-11-18T12:40:16.259+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: BEGIN in 0.024 seconds
[2024-11-18T12:40:16.260+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.263+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:40:16.264+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.264+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.265+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.266+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.266+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_risk__dbt_tmp"
[2024-11-18T12:40:16.267+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.267+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.268+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:40:16.268+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.268+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:40:16.269+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.269+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.270+0000] {subprocess.py:106} INFO - -- Create the patient risk dimension table in the analytics_schema
[2024-11-18T12:40:16.270+0000] {subprocess.py:106} INFO - with dim_risk as (
[2024-11-18T12:40:16.271+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:16.271+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(patient_risk_profile as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as risk_id,
[2024-11-18T12:40:16.272+0000] {subprocess.py:106} INFO -         patient_risk_profile,  -- Ensure this column exists in the source table
[2024-11-18T12:40:16.272+0000] {subprocess.py:106} INFO -         count(*) as cases,
[2024-11-18T12:40:16.273+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:40:16.273+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:40:16.274+0000] {subprocess.py:106} INFO -     group by risk_id, patient_risk_profile
[2024-11-18T12:40:16.274+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:40:16.275+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:16.276+0000] {subprocess.py:106} INFO -     risk_id,
[2024-11-18T12:40:16.277+0000] {subprocess.py:106} INFO -     patient_risk_profile,
[2024-11-18T12:40:16.277+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:40:16.278+0000] {subprocess.py:106} INFO - from dim_risk
[2024-11-18T12:40:16.279+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:40:16.280+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.307+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: SELECT 2 in 0.042 seconds
[2024-11-18T12:40:16.321+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.323+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:40:16.323+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_risk" rename to "dim_risk__dbt_backup"
[2024-11-18T12:40:16.325+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.337+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.339+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:40:16.340+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_risk__dbt_tmp" rename to "dim_risk"
[2024-11-18T12:40:16.342+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.349+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: COMMIT
[2024-11-18T12:40:16.350+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.352+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: COMMIT
[2024-11-18T12:40:16.357+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: COMMIT in 0.003 seconds
[2024-11-18T12:40:16.365+0000] {subprocess.py:106} INFO - [0m12:40:16  Applying DROP to: "db"."public_analytics_schema"."dim_risk__dbt_backup"
[2024-11-18T12:40:16.368+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:40:16.369+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:40:16.370+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_risk__dbt_backup" cascade
[2024-11-18T12:40:16.375+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: DROP TABLE in 0.005 seconds
[2024-11-18T12:40:16.380+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.dim_risk: Close
[2024-11-18T12:40:16.382+0000] {subprocess.py:106} INFO - [0m12:40:16  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a06d89f40>]}
[2024-11-18T12:40:16.384+0000] {subprocess.py:106} INFO - [0m12:40:16  4 of 6 OK created sql table model public_analytics_schema.dim_risk ............. [[32mSELECT 2[0m in 0.19s]
[2024-11-18T12:40:16.386+0000] {subprocess.py:106} INFO - [0m12:40:16  Finished running node model.etl_pipeline.dim_risk
[2024-11-18T12:40:16.387+0000] {subprocess.py:106} INFO - [0m12:40:16  Began running node model.etl_pipeline.fact_revenue
[2024-11-18T12:40:16.389+0000] {subprocess.py:106} INFO - [0m12:40:16  5 of 6 START sql table model public_analytics_schema.fact_revenue .............. [RUN]
[2024-11-18T12:40:16.390+0000] {subprocess.py:106} INFO - [0m12:40:16  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_risk, now model.etl_pipeline.fact_revenue)
[2024-11-18T12:40:16.392+0000] {subprocess.py:106} INFO - [0m12:40:16  Began compiling node model.etl_pipeline.fact_revenue
[2024-11-18T12:40:16.416+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing injected SQL for node "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.418+0000] {subprocess.py:106} INFO - [0m12:40:16  Began executing node model.etl_pipeline.fact_revenue
[2024-11-18T12:40:16.428+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing runtime sql for node "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.430+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.431+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: BEGIN
[2024-11-18T12:40:16.432+0000] {subprocess.py:106} INFO - [0m12:40:16  Opening a new connection, currently in state closed
[2024-11-18T12:40:16.452+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: BEGIN in 0.020 seconds
[2024-11-18T12:40:16.453+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.455+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:40:16.455+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.456+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.456+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.457+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.457+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."fact_revenue__dbt_tmp"
[2024-11-18T12:40:16.458+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.458+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.458+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:40:16.459+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.459+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:40:16.460+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.461+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.461+0000] {subprocess.py:106} INFO - -- Create the fact table for patient visits
[2024-11-18T12:40:16.462+0000] {subprocess.py:106} INFO - with fact_patient_visits as (
[2024-11-18T12:40:16.463+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:40:16.463+0000] {subprocess.py:106} INFO -         -- Surrogate keys from dimension tables
[2024-11-18T12:40:16.464+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(doctor as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as doctor_id,
[2024-11-18T12:40:16.465+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as branch_id,
[2024-11-18T12:40:16.465+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(patient_risk_profile as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as risk_id,
[2024-11-18T12:40:16.466+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.467+0000] {subprocess.py:106} INFO -         -- Natural keys and attributes
[2024-11-18T12:40:16.468+0000] {subprocess.py:106} INFO -         row_id,
[2024-11-18T12:40:16.468+0000] {subprocess.py:106} INFO -         patient_id,
[2024-11-18T12:40:16.469+0000] {subprocess.py:106} INFO -         department,
[2024-11-18T12:40:16.470+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.470+0000] {subprocess.py:106} INFO -         -- Metrics (facts)
[2024-11-18T12:40:16.471+0000] {subprocess.py:106} INFO -         revenue,
[2024-11-18T12:40:16.472+0000] {subprocess.py:106} INFO -         minutes_to_service,
[2024-11-18T12:40:16.472+0000] {subprocess.py:106} INFO -         number_of_patient_visits
[2024-11-18T12:40:16.473+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.473+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.474+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:40:16.474+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:40:16.475+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.475+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:40:16.476+0000] {subprocess.py:106} INFO -     -- Surrogate keys
[2024-11-18T12:40:16.476+0000] {subprocess.py:106} INFO -     doctor_id,
[2024-11-18T12:40:16.477+0000] {subprocess.py:106} INFO -     branch_id,
[2024-11-18T12:40:16.477+0000] {subprocess.py:106} INFO -     risk_id,
[2024-11-18T12:40:16.478+0000] {subprocess.py:106} INFO -     -- Natural keys
[2024-11-18T12:40:16.478+0000] {subprocess.py:106} INFO -     row_id,
[2024-11-18T12:40:16.479+0000] {subprocess.py:106} INFO -     patient_id,
[2024-11-18T12:40:16.479+0000] {subprocess.py:106} INFO -     -- Metrics
[2024-11-18T12:40:16.479+0000] {subprocess.py:106} INFO -     minutes_to_service,
[2024-11-18T12:40:16.480+0000] {subprocess.py:106} INFO -     number_of_patient_visits,
[2024-11-18T12:40:16.480+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:40:16.481+0000] {subprocess.py:106} INFO - from fact_patient_visits
[2024-11-18T12:40:16.481+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:40:16.482+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.535+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: SELECT 6334 in 0.079 seconds
[2024-11-18T12:40:16.544+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.545+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:40:16.546+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."fact_revenue" rename to "fact_revenue__dbt_backup"
[2024-11-18T12:40:16.547+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.556+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.557+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:40:16.558+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."fact_revenue__dbt_tmp" rename to "fact_revenue"
[2024-11-18T12:40:16.559+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.566+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: COMMIT
[2024-11-18T12:40:16.567+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.569+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: COMMIT
[2024-11-18T12:40:16.609+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: COMMIT in 0.039 seconds
[2024-11-18T12:40:16.619+0000] {subprocess.py:106} INFO - [0m12:40:16  Applying DROP to: "db"."public_analytics_schema"."fact_revenue__dbt_backup"
[2024-11-18T12:40:16.621+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:40:16.623+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:40:16.623+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."fact_revenue__dbt_backup" cascade
[2024-11-18T12:40:16.631+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: DROP TABLE in 0.007 seconds
[2024-11-18T12:40:16.636+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.fact_revenue: Close
[2024-11-18T12:40:16.639+0000] {subprocess.py:106} INFO - [0m12:40:16  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a06d8a240>]}
[2024-11-18T12:40:16.642+0000] {subprocess.py:106} INFO - [0m12:40:16  5 of 6 OK created sql table model public_analytics_schema.fact_revenue ......... [[32mSELECT 6334[0m in 0.25s]
[2024-11-18T12:40:16.646+0000] {subprocess.py:106} INFO - [0m12:40:16  Finished running node model.etl_pipeline.fact_revenue
[2024-11-18T12:40:16.648+0000] {subprocess.py:106} INFO - [0m12:40:16  Began running node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:40:16.650+0000] {subprocess.py:106} INFO - [0m12:40:16  6 of 6 START sql table model public.staging_patient_visits ..................... [RUN]
[2024-11-18T12:40:16.652+0000] {subprocess.py:106} INFO - [0m12:40:16  Re-using an available connection from the pool (formerly model.etl_pipeline.fact_revenue, now model.etl_pipeline.staging_patient_visits)
[2024-11-18T12:40:16.654+0000] {subprocess.py:106} INFO - [0m12:40:16  Began compiling node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:40:16.667+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing injected SQL for node "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.670+0000] {subprocess.py:106} INFO - [0m12:40:16  Began executing node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:40:16.686+0000] {subprocess.py:106} INFO - [0m12:40:16  Writing runtime sql for node "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.688+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.690+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: BEGIN
[2024-11-18T12:40:16.691+0000] {subprocess.py:106} INFO - [0m12:40:16  Opening a new connection, currently in state closed
[2024-11-18T12:40:16.719+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: BEGIN in 0.026 seconds
[2024-11-18T12:40:16.720+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.722+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:40:16.723+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.723+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.724+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.724+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.725+0000] {subprocess.py:106} INFO -   create  table "db"."public"."staging_patient_visits__dbt_tmp"
[2024-11-18T12:40:16.726+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.726+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.727+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:40:16.727+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.728+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:40:16.728+0000] {subprocess.py:106} INFO -     -- having staging_patient as table as it is temporary and has no updates
[2024-11-18T12:40:16.729+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.730+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.730+0000] {subprocess.py:106} INFO - -- loading the data extracted from the csv file
[2024-11-18T12:40:16.731+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.731+0000] {subprocess.py:106} INFO - SELECT *
[2024-11-18T12:40:16.732+0000] {subprocess.py:106} INFO - FROM "db"."public"."staging_patient_visits"
[2024-11-18T12:40:16.732+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:40:16.733+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:40:16.873+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: SELECT 6334 in 0.149 seconds
[2024-11-18T12:40:16.883+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.884+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:40:16.884+0000] {subprocess.py:106} INFO - alter table "db"."public"."staging_patient_visits" rename to "staging_patient_visits__dbt_backup"
[2024-11-18T12:40:16.886+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.895+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.896+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:40:16.897+0000] {subprocess.py:106} INFO - alter table "db"."public"."staging_patient_visits__dbt_tmp" rename to "staging_patient_visits"
[2024-11-18T12:40:16.898+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:40:16.904+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: COMMIT
[2024-11-18T12:40:16.905+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.907+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: COMMIT
[2024-11-18T12:40:16.951+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: COMMIT in 0.043 seconds
[2024-11-18T12:40:16.964+0000] {subprocess.py:106} INFO - [0m12:40:16  Applying DROP to: "db"."public"."staging_patient_visits__dbt_backup"
[2024-11-18T12:40:16.967+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:40:16.968+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:40:16.969+0000] {subprocess.py:106} INFO - drop table if exists "db"."public"."staging_patient_visits__dbt_backup" cascade
[2024-11-18T12:40:16.975+0000] {subprocess.py:106} INFO - [0m12:40:16  SQL status: DROP TABLE in 0.006 seconds
[2024-11-18T12:40:16.981+0000] {subprocess.py:106} INFO - [0m12:40:16  On model.etl_pipeline.staging_patient_visits: Close
[2024-11-18T12:40:16.983+0000] {subprocess.py:106} INFO - [0m12:40:16  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31eaa659-9f62-461e-a3c6-1a372486bc10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a0655fa10>]}
[2024-11-18T12:40:16.985+0000] {subprocess.py:106} INFO - [0m12:40:16  6 of 6 OK created sql table model public.staging_patient_visits ................ [[32mSELECT 6334[0m in 0.33s]
[2024-11-18T12:40:16.987+0000] {subprocess.py:106} INFO - [0m12:40:16  Finished running node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:40:16.991+0000] {subprocess.py:106} INFO - [0m12:40:16  Using postgres connection "master"
[2024-11-18T12:40:16.992+0000] {subprocess.py:106} INFO - [0m12:40:16  On master: BEGIN
[2024-11-18T12:40:16.993+0000] {subprocess.py:106} INFO - [0m12:40:16  Opening a new connection, currently in state closed
[2024-11-18T12:40:17.016+0000] {subprocess.py:106} INFO - [0m12:40:17  SQL status: BEGIN in 0.022 seconds
[2024-11-18T12:40:17.017+0000] {subprocess.py:106} INFO - [0m12:40:17  On master: COMMIT
[2024-11-18T12:40:17.019+0000] {subprocess.py:106} INFO - [0m12:40:17  Using postgres connection "master"
[2024-11-18T12:40:17.020+0000] {subprocess.py:106} INFO - [0m12:40:17  On master: COMMIT
[2024-11-18T12:40:17.022+0000] {subprocess.py:106} INFO - [0m12:40:17  SQL status: COMMIT in 0.000 seconds
[2024-11-18T12:40:17.023+0000] {subprocess.py:106} INFO - [0m12:40:17  On master: Close
[2024-11-18T12:40:17.025+0000] {subprocess.py:106} INFO - [0m12:40:17  Connection 'master' was properly closed.
[2024-11-18T12:40:17.026+0000] {subprocess.py:106} INFO - [0m12:40:17  Connection 'model.etl_pipeline.staging_patient_visits' was properly closed.
[2024-11-18T12:40:17.028+0000] {subprocess.py:106} INFO - [0m12:40:17
[2024-11-18T12:40:17.030+0000] {subprocess.py:106} INFO - [0m12:40:17  Finished running 6 table models in 0 hours 0 minutes and 2.11 seconds (2.11s).
[2024-11-18T12:40:17.035+0000] {subprocess.py:106} INFO - [0m12:40:17  Command end result
[2024-11-18T12:40:17.159+0000] {subprocess.py:106} INFO - [0m12:40:17
[2024-11-18T12:40:17.160+0000] {subprocess.py:106} INFO - [0m12:40:17  [32mCompleted successfully[0m
[2024-11-18T12:40:17.162+0000] {subprocess.py:106} INFO - [0m12:40:17
[2024-11-18T12:40:17.163+0000] {subprocess.py:106} INFO - [0m12:40:17  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[2024-11-18T12:40:17.165+0000] {subprocess.py:106} INFO - [0m12:40:17  Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.5199797, "process_user_time": 4.12952, "process_kernel_time": 0.263905, "process_mem_max_rss": "120932", "process_in_blocks": "288", "process_out_blocks": "3920"}
[2024-11-18T12:40:17.167+0000] {subprocess.py:106} INFO - [0m12:40:17  Command `dbt run` succeeded at 12:40:17.166531 after 3.52 seconds
[2024-11-18T12:40:17.168+0000] {subprocess.py:106} INFO - [0m12:40:17  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a09cfc710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a081b9070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7c6a083fc110>]}
[2024-11-18T12:40:17.170+0000] {subprocess.py:106} INFO - [0m12:40:17  Flushing usage events
[2024-11-18T12:40:18.787+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2024-11-18T12:40:18.822+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-18T12:40:18.823+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=etl_pipeline_dag, task_id=dbt_run, run_id=manual__2024-11-18T12:39:53.182771+00:00, execution_date=20241118T123953, start_date=20241118T124011, end_date=20241118T124018
[2024-11-18T12:40:18.871+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-11-18T12:40:18.913+0000] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-11-18T12:40:18.924+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
