[2024-11-18T12:42:20.578+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-18T12:42:20.592+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_pipeline_dag.dbt_run manual__2024-11-18T12:42:00.552416+00:00 [queued]>
[2024-11-18T12:42:20.601+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_pipeline_dag.dbt_run manual__2024-11-18T12:42:00.552416+00:00 [queued]>
[2024-11-18T12:42:20.602+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2024-11-18T12:42:20.627+0000] {taskinstance.py:2889} INFO - Executing <Task(BashOperator): dbt_run> on 2024-11-18 12:42:00.552416+00:00
[2024-11-18T12:42:20.634+0000] {standard_task_runner.py:72} INFO - Started process 337 to run task
[2024-11-18T12:42:20.638+0000] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'etl_pipeline_dag', 'dbt_run', 'manual__2024-11-18T12:42:00.552416+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpz4owloze']
[2024-11-18T12:42:20.639+0000] {standard_task_runner.py:105} INFO - Job 27: Subtask dbt_run
[2024-11-18T12:42:20.724+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_pipeline_dag.dbt_run manual__2024-11-18T12:42:00.552416+00:00 [running]> on host 945969587f65
[2024-11-18T12:42:20.940+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='dbt_run' AIRFLOW_CTX_EXECUTION_DATE='2024-11-18T12:42:00.552416+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-18T12:42:00.552416+00:00'
[2024-11-18T12:42:20.942+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-18T12:42:21.011+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2024-11-18T12:42:21.013+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', '\n            cd /opt/airflow/etl_pipeline &&             echo "Running dbt with profiles.yml in: $DBT_PROFILES_DIR" &&             dbt run --debug --profiles-dir /opt/airflow/etl_pipeline\n        ']
[2024-11-18T12:42:21.039+0000] {subprocess.py:99} INFO - Output:
[2024-11-18T12:42:21.045+0000] {subprocess.py:106} INFO - Running dbt with profiles.yml in:
[2024-11-18T12:42:22.939+0000] {subprocess.py:106} INFO - [0m12:42:22  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498bf00e00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498a6f1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498a6f32f0>]}
[2024-11-18T12:42:22.940+0000] {subprocess.py:106} INFO - [0m12:42:22  Running with dbt=1.8.8
[2024-11-18T12:42:22.940+0000] {subprocess.py:106} INFO - [0m12:42:22  running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/opt/airflow/etl_pipeline/logs', 'profiles_dir': '/opt/airflow/etl_pipeline', 'debug': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run --debug --profiles-dir /opt/airflow/etl_pipeline', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[2024-11-18T12:42:23.142+0000] {subprocess.py:106} INFO - [0m12:42:23  Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498ad0aab0>]}
[2024-11-18T12:42:23.214+0000] {subprocess.py:106} INFO - [0m12:42:23  Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498b93aa50>]}
[2024-11-18T12:42:23.215+0000] {subprocess.py:106} INFO - [0m12:42:23  Registered adapter: postgres=1.8.2
[2024-11-18T12:42:23.232+0000] {subprocess.py:106} INFO - [0m12:42:23  checksum: ec53e341a936509427e16df1c646386a84befec52c1957e3f4359fbc36fc93f5, vars: {}, profile: , target: , version: 1.8.8
[2024-11-18T12:42:23.406+0000] {subprocess.py:106} INFO - [0m12:42:23  Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[2024-11-18T12:42:23.406+0000] {subprocess.py:106} INFO - [0m12:42:23  Partial parsing enabled, no changes found, skipping parsing
[2024-11-18T12:42:23.413+0000] {subprocess.py:106} INFO - [0m12:42:23  [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
[2024-11-18T12:42:23.413+0000] {subprocess.py:106} INFO - There are 1 unused configuration paths:
[2024-11-18T12:42:23.414+0000] {subprocess.py:106} INFO - - models.etl_pipeline.production
[2024-11-18T12:42:23.466+0000] {subprocess.py:106} INFO - [0m12:42:23  Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774989f2b740>]}
[2024-11-18T12:42:23.598+0000] {subprocess.py:106} INFO - [0m12:42:23  Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498a027a10>]}
[2024-11-18T12:42:23.598+0000] {subprocess.py:106} INFO - [0m12:42:23  Found 6 models, 25 data tests, 2 sources, 539 macros
[2024-11-18T12:42:23.599+0000] {subprocess.py:106} INFO - [0m12:42:23  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774989fbeab0>]}
[2024-11-18T12:42:23.602+0000] {subprocess.py:106} INFO - [0m12:42:23
[2024-11-18T12:42:23.602+0000] {subprocess.py:106} INFO - [0m12:42:23  Acquiring new postgres connection 'master'
[2024-11-18T12:42:23.606+0000] {subprocess.py:106} INFO - [0m12:42:23  Acquiring new postgres connection 'list_db'
[2024-11-18T12:42:23.648+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "list_db"
[2024-11-18T12:42:23.648+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db"} */
[2024-11-18T12:42:23.649+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.649+0000] {subprocess.py:106} INFO -     select distinct nspname from pg_namespace
[2024-11-18T12:42:23.649+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.649+0000] {subprocess.py:106} INFO - [0m12:42:23  Opening a new connection, currently in state init
[2024-11-18T12:42:23.659+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: SELECT 6 in 0.010 seconds
[2024-11-18T12:42:23.662+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db: Close
[2024-11-18T12:42:23.667+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "list_db"
[2024-11-18T12:42:23.667+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db"} */
[2024-11-18T12:42:23.667+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.668+0000] {subprocess.py:106} INFO -     select distinct nspname from pg_namespace
[2024-11-18T12:42:23.668+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.668+0000] {subprocess.py:106} INFO - [0m12:42:23  Opening a new connection, currently in state closed
[2024-11-18T12:42:23.680+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: SELECT 6 in 0.012 seconds
[2024-11-18T12:42:23.682+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db: Close
[2024-11-18T12:42:23.687+0000] {subprocess.py:106} INFO - [0m12:42:23  Re-using an available connection from the pool (formerly list_db, now list_db_public)
[2024-11-18T12:42:23.701+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "list_db_public"
[2024-11-18T12:42:23.702+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public: BEGIN
[2024-11-18T12:42:23.703+0000] {subprocess.py:106} INFO - [0m12:42:23  Opening a new connection, currently in state closed
[2024-11-18T12:42:23.726+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: BEGIN in 0.022 seconds
[2024-11-18T12:42:23.728+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "list_db_public"
[2024-11-18T12:42:23.730+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db_public"} */
[2024-11-18T12:42:23.731+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:23.732+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:42:23.732+0000] {subprocess.py:106} INFO -       tablename as name,
[2024-11-18T12:42:23.733+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:42:23.734+0000] {subprocess.py:106} INFO -       'table' as type
[2024-11-18T12:42:23.735+0000] {subprocess.py:106} INFO -     from pg_tables
[2024-11-18T12:42:23.736+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:42:23.737+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:42:23.738+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:23.738+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:42:23.739+0000] {subprocess.py:106} INFO -       viewname as name,
[2024-11-18T12:42:23.739+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:42:23.740+0000] {subprocess.py:106} INFO -       'view' as type
[2024-11-18T12:42:23.740+0000] {subprocess.py:106} INFO -     from pg_views
[2024-11-18T12:42:23.741+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:42:23.742+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:42:23.742+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:23.743+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:42:23.743+0000] {subprocess.py:106} INFO -       matviewname as name,
[2024-11-18T12:42:23.744+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:42:23.745+0000] {subprocess.py:106} INFO -       'materialized_view' as type
[2024-11-18T12:42:23.746+0000] {subprocess.py:106} INFO -     from pg_matviews
[2024-11-18T12:42:23.746+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public'
[2024-11-18T12:42:23.747+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.748+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: SELECT 1 in 0.010 seconds
[2024-11-18T12:42:23.749+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public: ROLLBACK
[2024-11-18T12:42:23.751+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public: Close
[2024-11-18T12:42:23.753+0000] {subprocess.py:106} INFO - [0m12:42:23  Re-using an available connection from the pool (formerly list_db_public, now list_db_public_analytics_schema)
[2024-11-18T12:42:23.764+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "list_db_public_analytics_schema"
[2024-11-18T12:42:23.765+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public_analytics_schema: BEGIN
[2024-11-18T12:42:23.767+0000] {subprocess.py:106} INFO - [0m12:42:23  Opening a new connection, currently in state closed
[2024-11-18T12:42:23.797+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: BEGIN in 0.029 seconds
[2024-11-18T12:42:23.800+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "list_db_public_analytics_schema"
[2024-11-18T12:42:23.801+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public_analytics_schema: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "list_db_public_analytics_schema"} */
[2024-11-18T12:42:23.802+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:23.803+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:42:23.804+0000] {subprocess.py:106} INFO -       tablename as name,
[2024-11-18T12:42:23.804+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:42:23.805+0000] {subprocess.py:106} INFO -       'table' as type
[2024-11-18T12:42:23.805+0000] {subprocess.py:106} INFO -     from pg_tables
[2024-11-18T12:42:23.806+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:42:23.806+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:42:23.807+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:23.808+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:42:23.808+0000] {subprocess.py:106} INFO -       viewname as name,
[2024-11-18T12:42:23.809+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:42:23.809+0000] {subprocess.py:106} INFO -       'view' as type
[2024-11-18T12:42:23.810+0000] {subprocess.py:106} INFO -     from pg_views
[2024-11-18T12:42:23.811+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:42:23.812+0000] {subprocess.py:106} INFO -     union all
[2024-11-18T12:42:23.812+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:23.813+0000] {subprocess.py:106} INFO -       'db' as database,
[2024-11-18T12:42:23.813+0000] {subprocess.py:106} INFO -       matviewname as name,
[2024-11-18T12:42:23.814+0000] {subprocess.py:106} INFO -       schemaname as schema,
[2024-11-18T12:42:23.815+0000] {subprocess.py:106} INFO -       'materialized_view' as type
[2024-11-18T12:42:23.815+0000] {subprocess.py:106} INFO -     from pg_matviews
[2024-11-18T12:42:23.816+0000] {subprocess.py:106} INFO -     where schemaname ilike 'public_analytics_schema'
[2024-11-18T12:42:23.817+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.817+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: SELECT 5 in 0.009 seconds
[2024-11-18T12:42:23.820+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public_analytics_schema: ROLLBACK
[2024-11-18T12:42:23.822+0000] {subprocess.py:106} INFO - [0m12:42:23  On list_db_public_analytics_schema: Close
[2024-11-18T12:42:23.856+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "master"
[2024-11-18T12:42:23.858+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: BEGIN
[2024-11-18T12:42:23.859+0000] {subprocess.py:106} INFO - [0m12:42:23  Opening a new connection, currently in state init
[2024-11-18T12:42:23.888+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: BEGIN in 0.028 seconds
[2024-11-18T12:42:23.890+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "master"
[2024-11-18T12:42:23.892+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "connection_name": "master"} */
[2024-11-18T12:42:23.893+0000] {subprocess.py:106} INFO - with relation as (
[2024-11-18T12:42:23.894+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:42:23.895+0000] {subprocess.py:106} INFO -             pg_rewrite.ev_class as class,
[2024-11-18T12:42:23.895+0000] {subprocess.py:106} INFO -             pg_rewrite.oid as id
[2024-11-18T12:42:23.896+0000] {subprocess.py:106} INFO -         from pg_rewrite
[2024-11-18T12:42:23.897+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:42:23.898+0000] {subprocess.py:106} INFO -     class as (
[2024-11-18T12:42:23.899+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:42:23.900+0000] {subprocess.py:106} INFO -             oid as id,
[2024-11-18T12:42:23.901+0000] {subprocess.py:106} INFO -             relname as name,
[2024-11-18T12:42:23.901+0000] {subprocess.py:106} INFO -             relnamespace as schema,
[2024-11-18T12:42:23.902+0000] {subprocess.py:106} INFO -             relkind as kind
[2024-11-18T12:42:23.903+0000] {subprocess.py:106} INFO -         from pg_class
[2024-11-18T12:42:23.903+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:42:23.904+0000] {subprocess.py:106} INFO -     dependency as (
[2024-11-18T12:42:23.904+0000] {subprocess.py:106} INFO -         select distinct
[2024-11-18T12:42:23.905+0000] {subprocess.py:106} INFO -             pg_depend.objid as id,
[2024-11-18T12:42:23.906+0000] {subprocess.py:106} INFO -             pg_depend.refobjid as ref
[2024-11-18T12:42:23.906+0000] {subprocess.py:106} INFO -         from pg_depend
[2024-11-18T12:42:23.907+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:42:23.907+0000] {subprocess.py:106} INFO -     schema as (
[2024-11-18T12:42:23.908+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:42:23.909+0000] {subprocess.py:106} INFO -             pg_namespace.oid as id,
[2024-11-18T12:42:23.910+0000] {subprocess.py:106} INFO -             pg_namespace.nspname as name
[2024-11-18T12:42:23.910+0000] {subprocess.py:106} INFO -         from pg_namespace
[2024-11-18T12:42:23.911+0000] {subprocess.py:106} INFO -         where nspname != 'information_schema' and nspname not like 'pg\_%'
[2024-11-18T12:42:23.912+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:42:23.913+0000] {subprocess.py:106} INFO -     referenced as (
[2024-11-18T12:42:23.914+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:42:23.915+0000] {subprocess.py:106} INFO -             relation.id AS id,
[2024-11-18T12:42:23.916+0000] {subprocess.py:106} INFO -             referenced_class.name ,
[2024-11-18T12:42:23.917+0000] {subprocess.py:106} INFO -             referenced_class.schema ,
[2024-11-18T12:42:23.918+0000] {subprocess.py:106} INFO -             referenced_class.kind
[2024-11-18T12:42:23.919+0000] {subprocess.py:106} INFO -         from relation
[2024-11-18T12:42:23.920+0000] {subprocess.py:106} INFO -         join class as referenced_class on relation.class=referenced_class.id
[2024-11-18T12:42:23.921+0000] {subprocess.py:106} INFO -         where referenced_class.kind in ('r', 'v', 'm')
[2024-11-18T12:42:23.922+0000] {subprocess.py:106} INFO -     ),
[2024-11-18T12:42:23.923+0000] {subprocess.py:106} INFO -     relationships as (
[2024-11-18T12:42:23.924+0000] {subprocess.py:106} INFO -         select
[2024-11-18T12:42:23.924+0000] {subprocess.py:106} INFO -             referenced.name as referenced_name,
[2024-11-18T12:42:23.925+0000] {subprocess.py:106} INFO -             referenced.schema as referenced_schema_id,
[2024-11-18T12:42:23.926+0000] {subprocess.py:106} INFO -             dependent_class.name as dependent_name,
[2024-11-18T12:42:23.926+0000] {subprocess.py:106} INFO -             dependent_class.schema as dependent_schema_id,
[2024-11-18T12:42:23.927+0000] {subprocess.py:106} INFO -             referenced.kind as kind
[2024-11-18T12:42:23.927+0000] {subprocess.py:106} INFO -         from referenced
[2024-11-18T12:42:23.928+0000] {subprocess.py:106} INFO -         join dependency on referenced.id=dependency.id
[2024-11-18T12:42:23.928+0000] {subprocess.py:106} INFO -         join class as dependent_class on dependency.ref=dependent_class.id
[2024-11-18T12:42:23.929+0000] {subprocess.py:106} INFO -         where
[2024-11-18T12:42:23.929+0000] {subprocess.py:106} INFO -             (referenced.name != dependent_class.name or
[2024-11-18T12:42:23.930+0000] {subprocess.py:106} INFO -              referenced.schema != dependent_class.schema)
[2024-11-18T12:42:23.931+0000] {subprocess.py:106} INFO -     )
[2024-11-18T12:42:23.932+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:23.933+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:23.934+0000] {subprocess.py:106} INFO -         referenced_schema.name as referenced_schema,
[2024-11-18T12:42:23.935+0000] {subprocess.py:106} INFO -         relationships.referenced_name as referenced_name,
[2024-11-18T12:42:23.935+0000] {subprocess.py:106} INFO -         dependent_schema.name as dependent_schema,
[2024-11-18T12:42:23.936+0000] {subprocess.py:106} INFO -         relationships.dependent_name as dependent_name
[2024-11-18T12:42:23.937+0000] {subprocess.py:106} INFO -     from relationships
[2024-11-18T12:42:23.938+0000] {subprocess.py:106} INFO -     join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
[2024-11-18T12:42:23.939+0000] {subprocess.py:106} INFO -     join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
[2024-11-18T12:42:23.940+0000] {subprocess.py:106} INFO -     group by referenced_schema, referenced_name, dependent_schema, dependent_name
[2024-11-18T12:42:23.940+0000] {subprocess.py:106} INFO -     order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[2024-11-18T12:42:23.941+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: SELECT 0 in 0.018 seconds
[2024-11-18T12:42:23.941+0000] {subprocess.py:106} INFO - [0m12:42:23  Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498ade7050>]}
[2024-11-18T12:42:23.942+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: ROLLBACK
[2024-11-18T12:42:23.942+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "master"
[2024-11-18T12:42:23.943+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: BEGIN
[2024-11-18T12:42:23.944+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: BEGIN in 0.001 seconds
[2024-11-18T12:42:23.944+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: COMMIT
[2024-11-18T12:42:23.945+0000] {subprocess.py:106} INFO - [0m12:42:23  Using postgres connection "master"
[2024-11-18T12:42:23.946+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: COMMIT
[2024-11-18T12:42:23.946+0000] {subprocess.py:106} INFO - [0m12:42:23  SQL status: COMMIT in 0.001 seconds
[2024-11-18T12:42:23.947+0000] {subprocess.py:106} INFO - [0m12:42:23  On master: Close
[2024-11-18T12:42:23.948+0000] {subprocess.py:106} INFO - [0m12:42:23  Concurrency: 1 threads (target='dev')
[2024-11-18T12:42:23.949+0000] {subprocess.py:106} INFO - [0m12:42:23
[2024-11-18T12:42:23.954+0000] {subprocess.py:106} INFO - [0m12:42:23  Began running node model.etl_pipeline.dim_branches
[2024-11-18T12:42:23.956+0000] {subprocess.py:106} INFO - [0m12:42:23  1 of 6 START sql table model public_analytics_schema.dim_branches .............. [RUN]
[2024-11-18T12:42:23.958+0000] {subprocess.py:106} INFO - [0m12:42:23  Re-using an available connection from the pool (formerly list_db_public_analytics_schema, now model.etl_pipeline.dim_branches)
[2024-11-18T12:42:23.959+0000] {subprocess.py:106} INFO - [0m12:42:23  Began compiling node model.etl_pipeline.dim_branches
[2024-11-18T12:42:24.058+0000] {subprocess.py:106} INFO - [0m12:42:24  Writing injected SQL for node "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.060+0000] {subprocess.py:106} INFO - [0m12:42:24  Began executing node model.etl_pipeline.dim_branches
[2024-11-18T12:42:24.242+0000] {subprocess.py:106} INFO - [0m12:42:24  Writing runtime sql for node "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.244+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.246+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: BEGIN
[2024-11-18T12:42:24.248+0000] {subprocess.py:106} INFO - [0m12:42:24  Opening a new connection, currently in state closed
[2024-11-18T12:42:24.276+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: BEGIN in 0.028 seconds
[2024-11-18T12:42:24.278+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.280+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:42:24.281+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.282+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.283+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.285+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.285+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_branches__dbt_tmp"
[2024-11-18T12:42:24.286+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.286+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.287+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:42:24.288+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.288+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:42:24.289+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.290+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.290+0000] {subprocess.py:106} INFO - -- Create the doctors dimension table in the analytics_schema
[2024-11-18T12:42:24.291+0000] {subprocess.py:106} INFO - with dim_bra as (
[2024-11-18T12:42:24.292+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:24.293+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as branch_id,
[2024-11-18T12:42:24.293+0000] {subprocess.py:106} INFO -         hospital_branch,
[2024-11-18T12:42:24.294+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:42:24.295+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:42:24.296+0000] {subprocess.py:106} INFO -     group by branch_id, hospital_branch
[2024-11-18T12:42:24.297+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:42:24.298+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:24.299+0000] {subprocess.py:106} INFO -     branch_id,
[2024-11-18T12:42:24.300+0000] {subprocess.py:106} INFO -     hospital_branch,
[2024-11-18T12:42:24.301+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:42:24.301+0000] {subprocess.py:106} INFO - from dim_bra
[2024-11-18T12:42:24.302+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:42:24.303+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.333+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: SELECT 3 in 0.051 seconds
[2024-11-18T12:42:24.364+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.367+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:42:24.369+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_branches" rename to "dim_branches__dbt_backup"
[2024-11-18T12:42:24.371+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:24.387+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.388+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:42:24.389+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_branches__dbt_tmp" rename to "dim_branches"
[2024-11-18T12:42:24.391+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:24.488+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: COMMIT
[2024-11-18T12:42:24.490+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.491+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: COMMIT
[2024-11-18T12:42:24.501+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: COMMIT in 0.007 seconds
[2024-11-18T12:42:24.530+0000] {subprocess.py:106} INFO - [0m12:42:24  Applying DROP to: "db"."public_analytics_schema"."dim_branches__dbt_backup"
[2024-11-18T12:42:24.555+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_branches"
[2024-11-18T12:42:24.557+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_branches"} */
[2024-11-18T12:42:24.558+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_branches__dbt_backup" cascade
[2024-11-18T12:42:24.565+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: DROP TABLE in 0.006 seconds
[2024-11-18T12:42:24.575+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_branches: Close
[2024-11-18T12:42:24.582+0000] {subprocess.py:106} INFO - [0m12:42:24  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498bf83a70>]}
[2024-11-18T12:42:24.585+0000] {subprocess.py:106} INFO - [0m12:42:24  1 of 6 OK created sql table model public_analytics_schema.dim_branches ......... [[32mSELECT 3[0m in 0.62s]
[2024-11-18T12:42:24.588+0000] {subprocess.py:106} INFO - [0m12:42:24  Finished running node model.etl_pipeline.dim_branches
[2024-11-18T12:42:24.590+0000] {subprocess.py:106} INFO - [0m12:42:24  Began running node model.etl_pipeline.dim_doctors
[2024-11-18T12:42:24.592+0000] {subprocess.py:106} INFO - [0m12:42:24  2 of 6 START sql table model public_analytics_schema.dim_doctors ............... [RUN]
[2024-11-18T12:42:24.596+0000] {subprocess.py:106} INFO - [0m12:42:24  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_branches, now model.etl_pipeline.dim_doctors)
[2024-11-18T12:42:24.598+0000] {subprocess.py:106} INFO - [0m12:42:24  Began compiling node model.etl_pipeline.dim_doctors
[2024-11-18T12:42:24.616+0000] {subprocess.py:106} INFO - [0m12:42:24  Writing injected SQL for node "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.619+0000] {subprocess.py:106} INFO - [0m12:42:24  Began executing node model.etl_pipeline.dim_doctors
[2024-11-18T12:42:24.640+0000] {subprocess.py:106} INFO - [0m12:42:24  Writing runtime sql for node "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.643+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.645+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: BEGIN
[2024-11-18T12:42:24.647+0000] {subprocess.py:106} INFO - [0m12:42:24  Opening a new connection, currently in state closed
[2024-11-18T12:42:24.675+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: BEGIN in 0.028 seconds
[2024-11-18T12:42:24.678+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.680+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:42:24.681+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.682+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.682+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.683+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.684+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_doctors__dbt_tmp"
[2024-11-18T12:42:24.685+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.686+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.687+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:42:24.687+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.688+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:42:24.689+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.689+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.690+0000] {subprocess.py:106} INFO - -- Create the doctors dimension table in the analytics_schema
[2024-11-18T12:42:24.691+0000] {subprocess.py:106} INFO - with dim_doc as (
[2024-11-18T12:42:24.691+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:24.692+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(doctor as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(department as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as doctor_id,
[2024-11-18T12:42:24.693+0000] {subprocess.py:106} INFO -         doctor,
[2024-11-18T12:42:24.693+0000] {subprocess.py:106} INFO -         department,
[2024-11-18T12:42:24.694+0000] {subprocess.py:106} INFO -         count(*) as total_visits,
[2024-11-18T12:42:24.694+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:42:24.695+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:42:24.696+0000] {subprocess.py:106} INFO -     group by doctor_id, doctor, department
[2024-11-18T12:42:24.696+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:42:24.697+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:24.697+0000] {subprocess.py:106} INFO -     doctor_id,
[2024-11-18T12:42:24.698+0000] {subprocess.py:106} INFO -     doctor,
[2024-11-18T12:42:24.698+0000] {subprocess.py:106} INFO -     department,
[2024-11-18T12:42:24.699+0000] {subprocess.py:106} INFO -     total_visits,
[2024-11-18T12:42:24.699+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:42:24.700+0000] {subprocess.py:106} INFO - from dim_doc
[2024-11-18T12:42:24.701+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:42:24.701+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.741+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: SELECT 31 in 0.059 seconds
[2024-11-18T12:42:24.755+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.757+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:42:24.758+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_doctors" rename to "dim_doctors__dbt_backup"
[2024-11-18T12:42:24.760+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:24.773+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.775+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:42:24.776+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_doctors__dbt_tmp" rename to "dim_doctors"
[2024-11-18T12:42:24.778+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:24.786+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: COMMIT
[2024-11-18T12:42:24.789+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.790+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: COMMIT
[2024-11-18T12:42:24.796+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: COMMIT in 0.003 seconds
[2024-11-18T12:42:24.806+0000] {subprocess.py:106} INFO - [0m12:42:24  Applying DROP to: "db"."public_analytics_schema"."dim_doctors__dbt_backup"
[2024-11-18T12:42:24.809+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_doctors"
[2024-11-18T12:42:24.811+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_doctors"} */
[2024-11-18T12:42:24.812+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_doctors__dbt_backup" cascade
[2024-11-18T12:42:24.820+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: DROP TABLE in 0.007 seconds
[2024-11-18T12:42:24.826+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_doctors: Close
[2024-11-18T12:42:24.828+0000] {subprocess.py:106} INFO - [0m12:42:24  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774989401c40>]}
[2024-11-18T12:42:24.831+0000] {subprocess.py:106} INFO - [0m12:42:24  2 of 6 OK created sql table model public_analytics_schema.dim_doctors .......... [[32mSELECT 31[0m in 0.23s]
[2024-11-18T12:42:24.834+0000] {subprocess.py:106} INFO - [0m12:42:24  Finished running node model.etl_pipeline.dim_doctors
[2024-11-18T12:42:24.835+0000] {subprocess.py:106} INFO - [0m12:42:24  Began running node model.etl_pipeline.dim_patients
[2024-11-18T12:42:24.838+0000] {subprocess.py:106} INFO - [0m12:42:24  3 of 6 START sql table model public_analytics_schema.dim_patients .............. [RUN]
[2024-11-18T12:42:24.841+0000] {subprocess.py:106} INFO - [0m12:42:24  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_doctors, now model.etl_pipeline.dim_patients)
[2024-11-18T12:42:24.843+0000] {subprocess.py:106} INFO - [0m12:42:24  Began compiling node model.etl_pipeline.dim_patients
[2024-11-18T12:42:24.858+0000] {subprocess.py:106} INFO - [0m12:42:24  Writing injected SQL for node "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.861+0000] {subprocess.py:106} INFO - [0m12:42:24  Began executing node model.etl_pipeline.dim_patients
[2024-11-18T12:42:24.879+0000] {subprocess.py:106} INFO - [0m12:42:24  Writing runtime sql for node "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.882+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.883+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_patients: BEGIN
[2024-11-18T12:42:24.885+0000] {subprocess.py:106} INFO - [0m12:42:24  Opening a new connection, currently in state closed
[2024-11-18T12:42:24.915+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: BEGIN in 0.029 seconds
[2024-11-18T12:42:24.917+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.919+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:42:24.920+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.921+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.921+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.922+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.922+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_patients__dbt_tmp"
[2024-11-18T12:42:24.923+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.923+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.924+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:42:24.925+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.926+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:42:24.926+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.927+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.928+0000] {subprocess.py:106} INFO - -- Create the patients dimension table in the analytics_schema
[2024-11-18T12:42:24.929+0000] {subprocess.py:106} INFO - with dim_pat as (
[2024-11-18T12:42:24.930+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:24.931+0000] {subprocess.py:106} INFO -         patient_id,
[2024-11-18T12:42:24.932+0000] {subprocess.py:106} INFO -         patient_name,
[2024-11-18T12:42:24.933+0000] {subprocess.py:106} INFO -         count(*) as total_visits,
[2024-11-18T12:42:24.934+0000] {subprocess.py:106} INFO -         sum(revenue) as total_spent
[2024-11-18T12:42:24.935+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:42:24.936+0000] {subprocess.py:106} INFO -     group by patient_id, patient_name
[2024-11-18T12:42:24.937+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:42:24.938+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:24.939+0000] {subprocess.py:106} INFO -     patient_id,
[2024-11-18T12:42:24.939+0000] {subprocess.py:106} INFO -     patient_name,
[2024-11-18T12:42:24.940+0000] {subprocess.py:106} INFO -     total_visits,
[2024-11-18T12:42:24.941+0000] {subprocess.py:106} INFO -     total_spent
[2024-11-18T12:42:24.941+0000] {subprocess.py:106} INFO - from dim_pat
[2024-11-18T12:42:24.942+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:42:24.942+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:24.944+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: SELECT 2423 in 0.023 seconds
[2024-11-18T12:42:24.956+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.958+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:42:24.959+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_patients" rename to "dim_patients__dbt_backup"
[2024-11-18T12:42:24.962+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: ALTER TABLE in 0.002 seconds
[2024-11-18T12:42:24.976+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.978+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:42:24.979+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_patients__dbt_tmp" rename to "dim_patients"
[2024-11-18T12:42:24.981+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:24.990+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_patients: COMMIT
[2024-11-18T12:42:24.991+0000] {subprocess.py:106} INFO - [0m12:42:24  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:24.993+0000] {subprocess.py:106} INFO - [0m12:42:24  On model.etl_pipeline.dim_patients: COMMIT
[2024-11-18T12:42:24.999+0000] {subprocess.py:106} INFO - [0m12:42:24  SQL status: COMMIT in 0.004 seconds
[2024-11-18T12:42:25.009+0000] {subprocess.py:106} INFO - [0m12:42:25  Applying DROP to: "db"."public_analytics_schema"."dim_patients__dbt_backup"
[2024-11-18T12:42:25.012+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_patients"
[2024-11-18T12:42:25.014+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_patients: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_patients"} */
[2024-11-18T12:42:25.015+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_patients__dbt_backup" cascade
[2024-11-18T12:42:25.022+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: DROP TABLE in 0.005 seconds
[2024-11-18T12:42:25.027+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_patients: Close
[2024-11-18T12:42:25.031+0000] {subprocess.py:106} INFO - [0m12:42:25  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x774989401fd0>]}
[2024-11-18T12:42:25.033+0000] {subprocess.py:106} INFO - [0m12:42:25  3 of 6 OK created sql table model public_analytics_schema.dim_patients ......... [[32mSELECT 2423[0m in 0.19s]
[2024-11-18T12:42:25.036+0000] {subprocess.py:106} INFO - [0m12:42:25  Finished running node model.etl_pipeline.dim_patients
[2024-11-18T12:42:25.038+0000] {subprocess.py:106} INFO - [0m12:42:25  Began running node model.etl_pipeline.dim_risk
[2024-11-18T12:42:25.041+0000] {subprocess.py:106} INFO - [0m12:42:25  4 of 6 START sql table model public_analytics_schema.dim_risk .................. [RUN]
[2024-11-18T12:42:25.042+0000] {subprocess.py:106} INFO - [0m12:42:25  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_patients, now model.etl_pipeline.dim_risk)
[2024-11-18T12:42:25.045+0000] {subprocess.py:106} INFO - [0m12:42:25  Began compiling node model.etl_pipeline.dim_risk
[2024-11-18T12:42:25.063+0000] {subprocess.py:106} INFO - [0m12:42:25  Writing injected SQL for node "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.066+0000] {subprocess.py:106} INFO - [0m12:42:25  Began executing node model.etl_pipeline.dim_risk
[2024-11-18T12:42:25.090+0000] {subprocess.py:106} INFO - [0m12:42:25  Writing runtime sql for node "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.092+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.094+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: BEGIN
[2024-11-18T12:42:25.096+0000] {subprocess.py:106} INFO - [0m12:42:25  Opening a new connection, currently in state closed
[2024-11-18T12:42:25.125+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: BEGIN in 0.029 seconds
[2024-11-18T12:42:25.127+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.129+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:42:25.130+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.131+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.132+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.132+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.133+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."dim_risk__dbt_tmp"
[2024-11-18T12:42:25.134+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.134+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.135+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:42:25.136+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.137+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:42:25.138+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.139+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.140+0000] {subprocess.py:106} INFO - -- Create the patient risk dimension table in the analytics_schema
[2024-11-18T12:42:25.141+0000] {subprocess.py:106} INFO - with dim_risk as (
[2024-11-18T12:42:25.142+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:25.143+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(patient_risk_profile as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as risk_id,
[2024-11-18T12:42:25.143+0000] {subprocess.py:106} INFO -         patient_risk_profile,  -- Ensure this column exists in the source table
[2024-11-18T12:42:25.144+0000] {subprocess.py:106} INFO -         count(*) as cases,
[2024-11-18T12:42:25.145+0000] {subprocess.py:106} INFO -         sum(revenue) as revenue
[2024-11-18T12:42:25.145+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:42:25.146+0000] {subprocess.py:106} INFO -     group by risk_id, patient_risk_profile
[2024-11-18T12:42:25.147+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:42:25.147+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:25.148+0000] {subprocess.py:106} INFO -     risk_id,
[2024-11-18T12:42:25.148+0000] {subprocess.py:106} INFO -     patient_risk_profile,
[2024-11-18T12:42:25.149+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:42:25.150+0000] {subprocess.py:106} INFO - from dim_risk
[2024-11-18T12:42:25.150+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:42:25.151+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.181+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: SELECT 2 in 0.049 seconds
[2024-11-18T12:42:25.197+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.200+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:42:25.201+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_risk" rename to "dim_risk__dbt_backup"
[2024-11-18T12:42:25.203+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:25.216+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.218+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:42:25.220+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."dim_risk__dbt_tmp" rename to "dim_risk"
[2024-11-18T12:42:25.222+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:25.230+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: COMMIT
[2024-11-18T12:42:25.233+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.235+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: COMMIT
[2024-11-18T12:42:25.381+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: COMMIT in 0.144 seconds
[2024-11-18T12:42:25.394+0000] {subprocess.py:106} INFO - [0m12:42:25  Applying DROP to: "db"."public_analytics_schema"."dim_risk__dbt_backup"
[2024-11-18T12:42:25.398+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.dim_risk"
[2024-11-18T12:42:25.401+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.dim_risk"} */
[2024-11-18T12:42:25.401+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."dim_risk__dbt_backup" cascade
[2024-11-18T12:42:25.521+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: DROP TABLE in 0.118 seconds
[2024-11-18T12:42:25.526+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.dim_risk: Close
[2024-11-18T12:42:25.530+0000] {subprocess.py:106} INFO - [0m12:42:25  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498947cdd0>]}
[2024-11-18T12:42:25.533+0000] {subprocess.py:106} INFO - [0m12:42:25  4 of 6 OK created sql table model public_analytics_schema.dim_risk ............. [[32mSELECT 2[0m in 0.49s]
[2024-11-18T12:42:25.536+0000] {subprocess.py:106} INFO - [0m12:42:25  Finished running node model.etl_pipeline.dim_risk
[2024-11-18T12:42:25.538+0000] {subprocess.py:106} INFO - [0m12:42:25  Began running node model.etl_pipeline.fact_revenue
[2024-11-18T12:42:25.541+0000] {subprocess.py:106} INFO - [0m12:42:25  5 of 6 START sql table model public_analytics_schema.fact_revenue .............. [RUN]
[2024-11-18T12:42:25.542+0000] {subprocess.py:106} INFO - [0m12:42:25  Re-using an available connection from the pool (formerly model.etl_pipeline.dim_risk, now model.etl_pipeline.fact_revenue)
[2024-11-18T12:42:25.545+0000] {subprocess.py:106} INFO - [0m12:42:25  Began compiling node model.etl_pipeline.fact_revenue
[2024-11-18T12:42:25.575+0000] {subprocess.py:106} INFO - [0m12:42:25  Writing injected SQL for node "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.578+0000] {subprocess.py:106} INFO - [0m12:42:25  Began executing node model.etl_pipeline.fact_revenue
[2024-11-18T12:42:25.596+0000] {subprocess.py:106} INFO - [0m12:42:25  Writing runtime sql for node "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.601+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.602+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: BEGIN
[2024-11-18T12:42:25.604+0000] {subprocess.py:106} INFO - [0m12:42:25  Opening a new connection, currently in state closed
[2024-11-18T12:42:25.636+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: BEGIN in 0.031 seconds
[2024-11-18T12:42:25.638+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.640+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:42:25.641+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.642+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.643+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.643+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.644+0000] {subprocess.py:106} INFO -   create  table "db"."public_analytics_schema"."fact_revenue__dbt_tmp"
[2024-11-18T12:42:25.645+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.645+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.646+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:42:25.646+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.647+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:42:25.648+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.648+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.649+0000] {subprocess.py:106} INFO - -- Create the fact table for patient visits
[2024-11-18T12:42:25.650+0000] {subprocess.py:106} INFO - with fact_patient_visits as (
[2024-11-18T12:42:25.651+0000] {subprocess.py:106} INFO -     select
[2024-11-18T12:42:25.651+0000] {subprocess.py:106} INFO -         -- Surrogate keys from dimension tables
[2024-11-18T12:42:25.652+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(doctor as TEXT), '_dbt_utils_surrogate_key_null_') || '-' || coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as doctor_id,
[2024-11-18T12:42:25.653+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(hospital_branch as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as branch_id,
[2024-11-18T12:42:25.653+0000] {subprocess.py:106} INFO -         md5(cast(coalesce(cast(patient_risk_profile as TEXT), '_dbt_utils_surrogate_key_null_') as TEXT)) as risk_id,
[2024-11-18T12:42:25.654+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.655+0000] {subprocess.py:106} INFO -         -- Natural keys and attributes
[2024-11-18T12:42:25.656+0000] {subprocess.py:106} INFO -         row_id,
[2024-11-18T12:42:25.657+0000] {subprocess.py:106} INFO -         patient_id,
[2024-11-18T12:42:25.657+0000] {subprocess.py:106} INFO -         department,
[2024-11-18T12:42:25.658+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.658+0000] {subprocess.py:106} INFO -         -- Metrics (facts)
[2024-11-18T12:42:25.659+0000] {subprocess.py:106} INFO -         revenue,
[2024-11-18T12:42:25.660+0000] {subprocess.py:106} INFO -         minutes_to_service,
[2024-11-18T12:42:25.660+0000] {subprocess.py:106} INFO -         number_of_patient_visits
[2024-11-18T12:42:25.661+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.662+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.662+0000] {subprocess.py:106} INFO -     from "db"."public"."staging_patient_visits"
[2024-11-18T12:42:25.663+0000] {subprocess.py:106} INFO - )
[2024-11-18T12:42:25.663+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.664+0000] {subprocess.py:106} INFO - select
[2024-11-18T12:42:25.664+0000] {subprocess.py:106} INFO -     -- Surrogate keys
[2024-11-18T12:42:25.665+0000] {subprocess.py:106} INFO -     doctor_id,
[2024-11-18T12:42:25.666+0000] {subprocess.py:106} INFO -     branch_id,
[2024-11-18T12:42:25.666+0000] {subprocess.py:106} INFO -     risk_id,
[2024-11-18T12:42:25.667+0000] {subprocess.py:106} INFO -     -- Natural keys
[2024-11-18T12:42:25.667+0000] {subprocess.py:106} INFO -     row_id,
[2024-11-18T12:42:25.668+0000] {subprocess.py:106} INFO -     patient_id,
[2024-11-18T12:42:25.668+0000] {subprocess.py:106} INFO -     -- Metrics
[2024-11-18T12:42:25.669+0000] {subprocess.py:106} INFO -     minutes_to_service,
[2024-11-18T12:42:25.670+0000] {subprocess.py:106} INFO -     number_of_patient_visits,
[2024-11-18T12:42:25.670+0000] {subprocess.py:106} INFO -     revenue
[2024-11-18T12:42:25.671+0000] {subprocess.py:106} INFO - from fact_patient_visits
[2024-11-18T12:42:25.672+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:42:25.673+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:25.876+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: SELECT 6334 in 0.233 seconds
[2024-11-18T12:42:25.894+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.896+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:42:25.898+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."fact_revenue" rename to "fact_revenue__dbt_backup"
[2024-11-18T12:42:25.901+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: ALTER TABLE in 0.002 seconds
[2024-11-18T12:42:25.914+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.916+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:42:25.917+0000] {subprocess.py:106} INFO - alter table "db"."public_analytics_schema"."fact_revenue__dbt_tmp" rename to "fact_revenue"
[2024-11-18T12:42:25.919+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:25.928+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: COMMIT
[2024-11-18T12:42:25.930+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.931+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: COMMIT
[2024-11-18T12:42:25.960+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: COMMIT in 0.026 seconds
[2024-11-18T12:42:25.972+0000] {subprocess.py:106} INFO - [0m12:42:25  Applying DROP to: "db"."public_analytics_schema"."fact_revenue__dbt_backup"
[2024-11-18T12:42:25.975+0000] {subprocess.py:106} INFO - [0m12:42:25  Using postgres connection "model.etl_pipeline.fact_revenue"
[2024-11-18T12:42:25.978+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.fact_revenue"} */
[2024-11-18T12:42:25.979+0000] {subprocess.py:106} INFO - drop table if exists "db"."public_analytics_schema"."fact_revenue__dbt_backup" cascade
[2024-11-18T12:42:25.989+0000] {subprocess.py:106} INFO - [0m12:42:25  SQL status: DROP TABLE in 0.009 seconds
[2024-11-18T12:42:25.995+0000] {subprocess.py:106} INFO - [0m12:42:25  On model.etl_pipeline.fact_revenue: Close
[2024-11-18T12:42:25.999+0000] {subprocess.py:106} INFO - [0m12:42:25  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x77498934fbc0>]}
[2024-11-18T12:42:26.003+0000] {subprocess.py:106} INFO - [0m12:42:26  5 of 6 OK created sql table model public_analytics_schema.fact_revenue ......... [[32mSELECT 6334[0m in 0.46s]
[2024-11-18T12:42:26.005+0000] {subprocess.py:106} INFO - [0m12:42:26  Finished running node model.etl_pipeline.fact_revenue
[2024-11-18T12:42:26.007+0000] {subprocess.py:106} INFO - [0m12:42:26  Began running node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:42:26.010+0000] {subprocess.py:106} INFO - [0m12:42:26  6 of 6 START sql table model public.staging_patient_visits ..................... [RUN]
[2024-11-18T12:42:26.013+0000] {subprocess.py:106} INFO - [0m12:42:26  Re-using an available connection from the pool (formerly model.etl_pipeline.fact_revenue, now model.etl_pipeline.staging_patient_visits)
[2024-11-18T12:42:26.015+0000] {subprocess.py:106} INFO - [0m12:42:26  Began compiling node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:42:26.028+0000] {subprocess.py:106} INFO - [0m12:42:26  Writing injected SQL for node "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.031+0000] {subprocess.py:106} INFO - [0m12:42:26  Began executing node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:42:26.057+0000] {subprocess.py:106} INFO - [0m12:42:26  Writing runtime sql for node "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.060+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.062+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: BEGIN
[2024-11-18T12:42:26.064+0000] {subprocess.py:106} INFO - [0m12:42:26  Opening a new connection, currently in state closed
[2024-11-18T12:42:26.096+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: BEGIN in 0.031 seconds
[2024-11-18T12:42:26.098+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.101+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:42:26.102+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.103+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.104+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.104+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.105+0000] {subprocess.py:106} INFO -   create  table "db"."public"."staging_patient_visits__dbt_tmp"
[2024-11-18T12:42:26.106+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.107+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.108+0000] {subprocess.py:106} INFO -     as
[2024-11-18T12:42:26.108+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.109+0000] {subprocess.py:106} INFO -   (
[2024-11-18T12:42:26.110+0000] {subprocess.py:106} INFO -     -- having staging_patient as table as it is temporary and has no updates
[2024-11-18T12:42:26.112+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.112+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.113+0000] {subprocess.py:106} INFO - -- loading the data extracted from the csv file
[2024-11-18T12:42:26.114+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.114+0000] {subprocess.py:106} INFO - SELECT *
[2024-11-18T12:42:26.115+0000] {subprocess.py:106} INFO - FROM "db"."public"."staging_patient_visits"
[2024-11-18T12:42:26.115+0000] {subprocess.py:106} INFO -   );
[2024-11-18T12:42:26.116+0000] {subprocess.py:106} INFO - 
[2024-11-18T12:42:26.138+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: SELECT 6334 in 0.034 seconds
[2024-11-18T12:42:26.153+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.154+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:42:26.156+0000] {subprocess.py:106} INFO - alter table "db"."public"."staging_patient_visits" rename to "staging_patient_visits__dbt_backup"
[2024-11-18T12:42:26.159+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:26.179+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.181+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:42:26.182+0000] {subprocess.py:106} INFO - alter table "db"."public"."staging_patient_visits__dbt_tmp" rename to "staging_patient_visits"
[2024-11-18T12:42:26.186+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: ALTER TABLE in 0.001 seconds
[2024-11-18T12:42:26.197+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: COMMIT
[2024-11-18T12:42:26.199+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.201+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: COMMIT
[2024-11-18T12:42:26.208+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: COMMIT in 0.004 seconds
[2024-11-18T12:42:26.220+0000] {subprocess.py:106} INFO - [0m12:42:26  Applying DROP to: "db"."public"."staging_patient_visits__dbt_backup"
[2024-11-18T12:42:26.225+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "model.etl_pipeline.staging_patient_visits"
[2024-11-18T12:42:26.227+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: /* {"app": "dbt", "dbt_version": "1.8.8", "profile_name": "etl_pipeline", "target_name": "dev", "node_id": "model.etl_pipeline.staging_patient_visits"} */
[2024-11-18T12:42:26.228+0000] {subprocess.py:106} INFO - drop table if exists "db"."public"."staging_patient_visits__dbt_backup" cascade
[2024-11-18T12:42:26.237+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: DROP TABLE in 0.008 seconds
[2024-11-18T12:42:26.244+0000] {subprocess.py:106} INFO - [0m12:42:26  On model.etl_pipeline.staging_patient_visits: Close
[2024-11-18T12:42:26.248+0000] {subprocess.py:106} INFO - [0m12:42:26  Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '358e2eba-4a73-429e-b23f-de49ff0f91cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7749893990d0>]}
[2024-11-18T12:42:26.252+0000] {subprocess.py:106} INFO - [0m12:42:26  6 of 6 OK created sql table model public.staging_patient_visits ................ [[32mSELECT 6334[0m in 0.23s]
[2024-11-18T12:42:26.258+0000] {subprocess.py:106} INFO - [0m12:42:26  Finished running node model.etl_pipeline.staging_patient_visits
[2024-11-18T12:42:26.263+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "master"
[2024-11-18T12:42:26.265+0000] {subprocess.py:106} INFO - [0m12:42:26  On master: BEGIN
[2024-11-18T12:42:26.268+0000] {subprocess.py:106} INFO - [0m12:42:26  Opening a new connection, currently in state closed
[2024-11-18T12:42:26.301+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: BEGIN in 0.033 seconds
[2024-11-18T12:42:26.302+0000] {subprocess.py:106} INFO - [0m12:42:26  On master: COMMIT
[2024-11-18T12:42:26.304+0000] {subprocess.py:106} INFO - [0m12:42:26  Using postgres connection "master"
[2024-11-18T12:42:26.306+0000] {subprocess.py:106} INFO - [0m12:42:26  On master: COMMIT
[2024-11-18T12:42:26.309+0000] {subprocess.py:106} INFO - [0m12:42:26  SQL status: COMMIT in 0.001 seconds
[2024-11-18T12:42:26.312+0000] {subprocess.py:106} INFO - [0m12:42:26  On master: Close
[2024-11-18T12:42:26.315+0000] {subprocess.py:106} INFO - [0m12:42:26  Connection 'master' was properly closed.
[2024-11-18T12:42:26.317+0000] {subprocess.py:106} INFO - [0m12:42:26  Connection 'model.etl_pipeline.staging_patient_visits' was properly closed.
[2024-11-18T12:42:26.319+0000] {subprocess.py:106} INFO - [0m12:42:26
[2024-11-18T12:42:26.322+0000] {subprocess.py:106} INFO - [0m12:42:26  Finished running 6 table models in 0 hours 0 minutes and 2.72 seconds (2.72s).
[2024-11-18T12:42:26.333+0000] {subprocess.py:106} INFO - [0m12:42:26  Command end result
[2024-11-18T12:42:26.614+0000] {subprocess.py:106} INFO - [0m12:42:26
[2024-11-18T12:42:26.615+0000] {subprocess.py:106} INFO - [0m12:42:26  [32mCompleted successfully[0m
[2024-11-18T12:42:26.616+0000] {subprocess.py:106} INFO - [0m12:42:26
[2024-11-18T12:42:26.617+0000] {subprocess.py:106} INFO - [0m12:42:26  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[2024-11-18T12:42:26.618+0000] {subprocess.py:106} INFO - [0m12:42:26  Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.7313335, "process_user_time": 4.370981, "process_kernel_time": 0.261639, "process_mem_max_rss": "116384", "process_out_blocks": "2728", "process_in_blocks": "0"}
[2024-11-18T12:42:26.619+0000] {subprocess.py:106} INFO - [0m12:42:26  Command `dbt run` succeeded at 12:42:26.618666 after 3.73 seconds
[2024-11-18T12:42:26.619+0000] {subprocess.py:106} INFO - [0m12:42:26  Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7749894ab6e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7749894a9490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7749894aa240>]}
[2024-11-18T12:42:26.620+0000] {subprocess.py:106} INFO - [0m12:42:26  Flushing usage events
[2024-11-18T12:42:28.979+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2024-11-18T12:42:29.023+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-18T12:42:29.023+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=etl_pipeline_dag, task_id=dbt_run, run_id=manual__2024-11-18T12:42:00.552416+00:00, execution_date=20241118T124200, start_date=20241118T124220, end_date=20241118T124229
[2024-11-18T12:42:29.071+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-11-18T12:42:29.127+0000] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-11-18T12:42:29.151+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
